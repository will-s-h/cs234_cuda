{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhiv/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    level_1: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12157\n",
      "    })\n",
      "    level_2: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12938\n",
      "    })\n",
      "    level_3: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 5520\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the dataset name and the cache directory\n",
    "dataset_name = \"SakanaAI/AI-CUDA-Engineer-Archive\"\n",
    "cache_dir = \"./cache_dir\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "\n",
    "# Print the dataset to verify\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = dataset[\"level_1\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <torch/extension.h>\n",
      "\n",
      "#include <cuda.h>\n",
      "#include <cuda_runtime.h>\n",
      "#include <c10/cuda/CUDAException.h>\n",
      "\n",
      "#define TILE_SIZE 16\n",
      "\n",
      "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
      "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
      "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
      "\n",
      "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
      "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
      "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
      "\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "\n",
      "    int row = blockIdx.y * TILE_SIZE + ty;\n",
      "    int col = blockIdx.x * TILE_SIZE + tx;\n",
      "\n",
      "    float C_value = 0.0f;\n",
      "\n",
      "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
      "        // Load tiles into shared memory\n",
      "        if (row < N && m * TILE_SIZE + tx < N)\n",
      "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
      "        else\n",
      "            As[ty][tx] = 0.0f;\n",
      "\n",
      "        if (col < N && m * TILE_SIZE + ty < N)\n",
      "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
      "        else\n",
      "            Bs[ty][tx] = 0.0f;\n",
      "\n",
      "        __syncthreads();\n",
      "\n",
      "        // Compute partial product\n",
      "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
      "            C_value += As[ty][k] * Bs[k][tx];\n",
      "        }\n",
      "\n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    // Write the result\n",
      "    if (row < N && col < N)\n",
      "        C[row * N + col] = C_value;\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    CHECK_INPUT(A);\n",
      "    CHECK_INPUT(B);\n",
      "    CHECK_FLOAT(A);\n",
      "    CHECK_FLOAT(B);\n",
      "\n",
      "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
      "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
      "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
      "\n",
      "    int64_t N = A.size(0);\n",
      "\n",
      "    auto C = torch::zeros({N, N}, A.options());\n",
      "\n",
      "    const float* A_data = A.data_ptr<float>();\n",
      "    const float* B_data = B.data_ptr<float>();\n",
      "    float* C_data = C.data_ptr<float>();\n",
      "\n",
      "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
      "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
      "\n",
      "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
      "\n",
      "    // Check for kernel launch errors\n",
      "    C10_CUDA_CHECK(cudaGetLastError());\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\n",
      "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
      "    m.def(\"forward\", &forward, \"Matrix multiplication kernel (CUDA)\");\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(df_l1.iloc[0][[\"CUDA_Code\"]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_l1.iloc[0][[\"PyTorch_Code_Functional\"]].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_q14 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "model_q7 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "# model_q1 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_q7,torch_dtype=\"auto\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_q7, torch_dtype=\"auto\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "with torch.cuda.device(0):  # explicitly set GPU 0 if needed\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "# tokenizer = tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Decode the generated tokens to get the response\u001b[39;00m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:275\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    274\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:222\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    220\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    221\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 222\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"what is the solution of x^2 - 2x + 1 = 0?<think>\"\n",
    "\n",
    "# prompt = \"what is the second planet from the Sun?<think>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_length=10_000)\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = inputs.to('cpu')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n\\ndef module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs a single square matrix multiplication (C = A * B).\\n\\n    Args:\\n        A (torch.Tensor): Input matrix A of shape (N, N).\\n        B (torch.Tensor): Input matrix B of shape (N, N).\\n\\n    Returns:\\n        torch.Tensor: Output matrix C of shape (N, N).\\n    \"\"\"\\n    return torch.matmul(A, B)\\n\\n\\nclass Model(nn.Module):\\n    \"\"\"\\n    Simple model that performs a single square matrix multiplication (C = A * B)\\n    \"\"\"\\n\\n    def __init__(self):\\n        super(Model, self).__init__()\\n\\n    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\\n        return fn(A, B)\\n\\n\\nN = 2048\\n\\n\\ndef get_inputs():\\n    A = torch.randn(N, N)\\n    B = torch.randn(N, N)\\n    return [A, B]\\n\\n\\ndef get_init_inputs():\\n    return []  # No special initialization inputs needed\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_l1.iloc[0].PyTorch_Code_Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# CUDA Prompt\n",
    "############################################\n",
    "# PROBLEM_STATEMENT = \"\"\"You write custom CUDA kernels to replace the pytorch operators in the given architecture to get speedups. \\n\n",
    "#     You have complete freedom to choose the set of operators you want to replace. You may make the decision to replace some operators with custom CUDA kernels and leave others unchanged. You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\\n\n",
    "# \"\"\"\n",
    "# PROBLEM_INSTRUCTION = \"\"\"\n",
    "# Optimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \\n\n",
    "# \"\"\"\n",
    "PROBLEM_STATEMENT = \"\"\"You write custom CUDA kernels to replace the pytorch operators in the given architecture to get speedups. \\n\n",
    "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\\n\n",
    "\"\"\"\n",
    "PROBLEM_INSTRUCTION = \"\"\"\n",
    "Optimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \\n\n",
    "Be sure to have the correct names for the cuda_source and cpp_source strings. In the ModelNew implementation, be sure to call the cuda function inside of the cuda module, not just the cuda module.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt_generate_custom_cuda(\n",
    "    arc_src: str, example_arch_src: str, example_new_arch_src: str\n",
    ") -> str:\n",
    "    prompt = PROBLEM_STATEMENT\n",
    "\n",
    "    if example_arch_src != \"\" and example_new_arch_src != \"\":\n",
    "        prompt += f\"\"\"\n",
    "        Here's an example to show you the syntax of the architecture you will see implemented in torch: The example architecture is for element-wise addition: \\n\n",
    "        ``` \\n\n",
    "        {example_arch_src}\n",
    "        ``` \\n\n",
    "        The example new arch with custom CUDA kernels looks like this: \n",
    "        ```\n",
    "        {example_new_arch_src}\n",
    "        ``` \\n\n",
    "        \"\"\"\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "    You are given the following torch architecture: \\n\n",
    "    ```\n",
    "    {arc_src}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    prompt += PROBLEM_INSTRUCTION\n",
    "    return prompt\n",
    "\n",
    "def prompt_generate_custom_cuda_from_prompt_template(ref_arch_src: str) -> str:\n",
    "    \"\"\"\n",
    "    Using prompt example (an element-wise addition) for prompt templates\n",
    "    The most basic form of example just to show LLM the task and the expected output format\n",
    "    \"\"\"\n",
    "    arch = ref_arch_src\n",
    "    # These are strictly defined for now\n",
    "\n",
    "    # # path to prompt template, show an example of Model (torch specifications) and ModelNew (torch + custom CUDA kernels)\n",
    "    # example_arch_path = os.path.join(\n",
    "    #     REPO_TOP_PATH, f\"src/prompts/model_ex_add.py\"\n",
    "    # )\n",
    "    # example_new_arch_path = os.path.join(\n",
    "    #     REPO_TOP_PATH, f\"src/prompts/model_new_ex_add.py\"\n",
    "    # )\n",
    "\n",
    "    # if not os.path.exists(example_arch_path):\n",
    "    #     raise FileNotFoundError(\n",
    "    #         f\"Example architecture file not found: {example_arch_path}\"\n",
    "    #     )\n",
    "    # if not os.path.exists(example_new_arch_path):\n",
    "    #     raise FileNotFoundError(\n",
    "    #         f\"Example new architecture file not found: {example_new_arch_path}\"\n",
    "    #     )\n",
    "\n",
    "    example_arch = '''\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, a, b):\n",
    "            return a + b\n",
    "\n",
    "\n",
    "    def get_inputs():\n",
    "        # randomly generate input tensors based on the model architecture\n",
    "        a = torch.randn(1, 128).cuda()\n",
    "        b = torch.randn(1, 128).cuda()\n",
    "        return [a, b]\n",
    "\n",
    "\n",
    "    def get_init_inputs():\n",
    "        # randomly generate tensors required for initialization based on the model architecture\n",
    "        return []\n",
    "'''\n",
    "    example_new_arch = '''\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "    try:\n",
    "        # Define the custom CUDA kernel for element-wise addition\n",
    "        elementwise_add_source = \"\"\"\n",
    "        #include <torch/extension.h>\n",
    "        #include <cuda_runtime.h>\n",
    "\n",
    "        __global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
    "            int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (idx < size) {\n",
    "                out[idx] = a[idx] + b[idx];\n",
    "            }\n",
    "        }\n",
    "\n",
    "        torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "            auto size = a.numel();\n",
    "            auto out = torch::zeros_like(a);\n",
    "\n",
    "            const int block_size = 256;\n",
    "            const int num_blocks = (size + block_size - 1) / block_size;\n",
    "\n",
    "            elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
    "\n",
    "            return out;\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        elementwise_add_cpp_source = (\n",
    "            \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    "        )\n",
    "\n",
    "        # Compile the inline CUDA code for element-wise addition\n",
    "        elementwise_add = load_inline(\n",
    "            name=\"elementwise_add\",\n",
    "            cpp_sources=elementwise_add_cpp_source,\n",
    "            cuda_sources=elementwise_add_source,\n",
    "            functions=[\"elementwise_add_cuda\"],\n",
    "            verbose=True,\n",
    "            extra_cflags=[\"\"],\n",
    "            extra_ldflags=[\"\"],\n",
    "        )\n",
    "        \n",
    "        class ModelNew(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.elementwise_add = elementwise_add\n",
    "\n",
    "        def forward(self, a, b):\n",
    "            return self.elementwise_add.elementwise_add_cuda(a, b)\n",
    "    except Exception as e:\n",
    "        print(\"CUDA Kernel had error loading in. Probably an error in the outputted code\")\n",
    "        works=False\n",
    "\n",
    "'''\n",
    "\n",
    "    return prompt_generate_custom_cuda(arch, example_arch, example_new_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "functional_str = df_l1.iloc[0].PyTorch_Code_Functional\n",
    "prompt = prompt_generate_custom_cuda_from_prompt_template(functional_str)\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "# Tokenize the input promptb\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=10_000)\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# inputs = inputs.to('cpu')\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[response.find(\"</think>\")+len(\"</think>\"):])\n",
    "output = response[response.find(\"</think>\")+len(\"</think>\"):]\n",
    "output = output[output.find('import torch'):output.find('```', output.find('import torch'))]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Square_matrix_multiplication_\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.cpp_extension import load_inline\n",
      "\n",
      "try:\n",
      "    # Define the custom CUDA kernel for matrix multiplication\n",
      "    matmul_kernel_source = \"\"\"\n",
      "    #include <torch/extension.h>\n",
      "    #include <cuda_runtime.h>\n",
      "\n",
      "    __global__ void matmul_kernel(const float* a, const float* b, float* out, int n) {\n",
      "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "        float sum = 0.0f;\n",
      "        for (int k = 0; k < n; k++) {\n",
      "            sum += a[row * n + k] * b[k * n + col];\n",
      "        }\n",
      "        out[row * n + col] = sum;\n",
      "    }\n",
      "\n",
      "    torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {\n",
      "        int n = a.size(1);\n",
      "        int m = b.size(0);\n",
      "        int k = b.size(1);\n",
      "\n",
      "        auto out_size = m * k;\n",
      "        auto out = torch::zeros(m, k, sizeof(float));\n",
      "\n",
      "        const int block_size = 256;\n",
      "        const int num_blocks = (out_size + block_size - 1) / block_size;\n",
      "\n",
      "        matmul_kernel<<<num_blocks, block_size>>>(\n",
      "            a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), n);\n",
      "\n",
      "        return out;\n",
      "    }\n",
      "    \"\"\"\n",
      "\n",
      "    matmul_cpp_source = (\n",
      "        \"torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);\"\n",
      "    )\n",
      "\n",
      "    # Compile the inline CUDA code for matrix multiplication\n",
      "    matmul = load_inline(\n",
      "        name=\"matmul\",\n",
      "        cpp_sources=matmul_cpp_source,\n",
      "        cuda_sources=matmul_kernel_source,\n",
      "        functions=[\"matmul_cuda\"],\n",
      "        verbose=True,\n",
      "        extra_cflags=[\"-O3\"],\n",
      "        extra_ldflags=[\"\"],\n",
      "    )\n",
      "\n",
      "    class ModelNew(nn.Module):\n",
      "        def __init__(self) -> None:\n",
      "            super().__init__()\n",
      "\n",
      "        def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "            return matmul(A, B)\n",
      "\n",
      "    model = ModelNew()\n",
      "    return model\n",
      "\n",
      "except Exception as e:\n",
      "    print(\"CUDA Kernel had error loading in. Probably an error in the outputted code\")\n",
      "    return None\n",
      "\n",
      "CUDA did not compile correctly, syntax error, etc.\n",
      "2_Standard_matrix_multiplication_\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.cpp_extension import load_inline\n",
      "\n",
      "try:\n",
      "    # Define the custom CUDA kernel for matrix multiplication\n",
      "    matmul_source = \"\"\"\n",
      "    #include <torch/extension.h>\n",
      "    #include <cuda_runtime.h>\n",
      "\n",
      "    __global__ void matmul_kernel(const float* a, const float* b, float* out, int M, int K, int N) {\n",
      "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "        float sum = 0.0f;\n",
      "        for (int k = 0; k < K; ++k) {\n",
      "            sum += a[row * K + k] * b[k * N + col];\n",
      "        }\n",
      "        out[row * N + col] = sum;\n",
      "    }\n",
      "\n",
      "    torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {\n",
      "        int M = a.size(0);\n",
      "        int K = a.size(1);\n",
      "        int N = b.size(1);\n",
      "        \n",
      "        auto out_size = M * N;\n",
      "        auto out = torch::zeros_like(a, out_size);\n",
      "        \n",
      "        const int block_size = 256;\n",
      "        const int num_blocks = (out_size + block_size - 1) / block_size;\n",
      "\n",
      "        matmul_kernel<<<num_blocks, block_size>>>(\n",
      "            a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), M, K, N);\n",
      "\n",
      "        return out;\n",
      "    }\n",
      "    \"\"\"\n",
      "\n",
      "    matmul_cpp_source = (\n",
      "        \"torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);\"\n",
      "    )\n",
      "\n",
      "    # Compile the inline CUDA code for matrix multiplication\n",
      "    matmul = load_inline(\n",
      "        name=\"matmul\",\n",
      "        cpp_sources=matmul_cpp_source,\n",
      "        cuda_sources=matmul_source,\n",
      "        functions=[\"matmul_cuda\"],\n",
      "        verbose=True,\n",
      "        extra_cflags=[\"-O3\"],\n",
      "        extra_ldflags=[\"\"],\n",
      "    )\n",
      "\n",
      "    class ModelNew(nn.Module):\n",
      "        def __init__(self) -> None:\n",
      "            super().__init__()\n",
      "\n",
      "        def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "            return matmul(A, B)\n",
      "\n",
      "    M = 1024\n",
      "    K = 4096\n",
      "    N = 2048\n",
      "\n",
      "    def get_inputs():\n",
      "        A = torch.randn(M, K)\n",
      "        B = torch.randn(K, N)\n",
      "        return [A, B]\n",
      "\n",
      "    def get_init_inputs():\n",
      "        return []  # No special initialization inputs needed\n",
      "\n",
      "except Exception as e:\n",
      "    print(\"CUDA Kernel had error loading in. Probably an error in the outputted code\")\n",
      "    works=False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhiv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "The input conditions for extension module matmul have changed. Bumping to version 5 and re-building as matmul_v5...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/abhiv/.cache/torch_extensions/py310_cu124/matmul/build.ninja...\n",
      "/home/abhiv/miniconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module matmul_v5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=matmul_v5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/abhiv/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /home/abhiv/.cache/torch_extensions/py310_cu124/matmul/cuda.cu -o cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mcuda.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=matmul_v5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/abhiv/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /home/abhiv/.cache/torch_extensions/py310_cu124/matmul/cuda.cu -o cuda.cuda.o \n",
      "/home/abhiv/.cache/torch_extensions/py310_cu124/matmul/cuda.cu(24): error: no suitable constructor exists to convert from \"int\" to \"c10::TensorOptions\"\n",
      "          auto out = torch::zeros_like(a, out_size);\n",
      "                                          ^\n",
      "\n",
      "1 error detected in the compilation of \"/home/abhiv/.cache/torch_extensions/py310_cu124/matmul/cuda.cu\".\n",
      "[2/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=matmul_v5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/abhiv/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/abhiv/.cache/torch_extensions/py310_cu124/matmul/main.cpp -o main.o \n",
      "ninja: build stopped: subcommand failed.\n",
      "CUDA Kernel had error loading in. Probably an error in the outputted code\n"
     ]
    }
   ],
   "source": [
    "l1_samples = df_l1[df_l1.Kernel_Name == df_l1.Op_Name]\n",
    "num_correct= 0\n",
    "for idx, row in l1_samples.iterrows():\n",
    "    if  row.Task_ID > 2:\n",
    "        break\n",
    "    func_str = row.PyTorch_Code_Functional\n",
    "    prompt = prompt_generate_custom_cuda_from_prompt_template(func_str)\n",
    "    # Tokenize the input promptb\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # Generate the model's response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10_000)\n",
    "    # Decode the generated tokens to get the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    output = response[response.find(\"</think>\")+len(\"</think>\"):]\n",
    "    # print(func_str, output)\n",
    "    print(row.Op_Name)\n",
    "    output = output[output.find('import torch'):output.find('```', output.find('import torch'))]\n",
    "    print(output)\n",
    "\n",
    "    exec(func_str)\n",
    "    inputs = get_inputs()\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].cuda()\n",
    "    works=True\n",
    "    try:\n",
    "        exec(output)\n",
    "    except Exception as e:\n",
    "        print(\"CUDA did not compile correctly, syntax error, etc.\")\n",
    "        # print(e)\n",
    "        works=False\n",
    "    if not works:\n",
    "        continue\n",
    "    try:\n",
    "        torchmod = Model()\n",
    "        cudamod = ModelNew()\n",
    "        print(torch.allclose(torchmod.forward(*inputs), cudamod.forward(*inputs), rtol=1e-1, atol=1e-3))\n",
    "        if torch.allclose(torchmod.forward(*inputs), cudamod.forward(*inputs), rtol=1e-1, atol=1e-3):\n",
    "            num_correct += 1\n",
    "    except Exception as e:\n",
    "        print(\"CUDA compiled but model execution had an error\")\n",
    "        # print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Op_Name</th>\n",
       "      <th>Level_ID</th>\n",
       "      <th>Task_ID</th>\n",
       "      <th>Kernel_Name</th>\n",
       "      <th>CUDA_Runtime</th>\n",
       "      <th>PyTorch_Native_Runtime</th>\n",
       "      <th>PyTorch_Compile_Runtime</th>\n",
       "      <th>CUDA_Speedup_Native</th>\n",
       "      <th>CUDA_Speedup_Compile</th>\n",
       "      <th>CUDA_Code</th>\n",
       "      <th>PyTorch_Code_Module</th>\n",
       "      <th>PyTorch_Code_Functional</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Max_Diff</th>\n",
       "      <th>Error</th>\n",
       "      <th>NCU_Profile</th>\n",
       "      <th>Torch_Profile</th>\n",
       "      <th>Clang_Tidy</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_Square_matrix_multiplication_</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1_Square_matrix_multiplication_</td>\n",
       "      <td>2.115</td>\n",
       "      <td>0.421087</td>\n",
       "      <td>0.445168</td>\n",
       "      <td>0.199096</td>\n",
       "      <td>0.210481</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n\\n#include &lt;cuda...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\n\\nclass...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2_Standard_matrix_multiplication_</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2_Standard_matrix_multiplication_</td>\n",
       "      <td>1.942</td>\n",
       "      <td>0.425295</td>\n",
       "      <td>0.459274</td>\n",
       "      <td>0.218998</td>\n",
       "      <td>0.236495</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda_r...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3_Batched_matrix_multiplication</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3_Batched_matrix_multiplication</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.181113</td>\n",
       "      <td>0.159070</td>\n",
       "      <td>0.224150</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>4_Matrix_vector_multiplication_</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4_Matrix_vector_multiplication_</td>\n",
       "      <td>18.158</td>\n",
       "      <td>0.061012</td>\n",
       "      <td>0.169859</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.012</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>5_Matrix_scalar_multiplication</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5_Matrix_scalar_multiplication</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.181897</td>\n",
       "      <td>0.402290</td>\n",
       "      <td>0.794311</td>\n",
       "      <td>1.756726</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;ATen/c...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11152</th>\n",
       "      <td>96_HuberLoss</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>96_HuberLoss</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>0.053737</td>\n",
       "      <td>1.346388</td>\n",
       "      <td>4.478060</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11273</th>\n",
       "      <td>97_CosineSimilarityLoss</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>97_CosineSimilarityLoss</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.076431</td>\n",
       "      <td>0.054744</td>\n",
       "      <td>5.459388</td>\n",
       "      <td>3.910286</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11394</th>\n",
       "      <td>98_KLDivLoss</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>98_KLDivLoss</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.035177</td>\n",
       "      <td>2.223909</td>\n",
       "      <td>2.512667</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.007</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>99_TripletMarginLoss</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>99_TripletMarginLoss</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.050352</td>\n",
       "      <td>0.034053</td>\n",
       "      <td>2.517614</td>\n",
       "      <td>1.702653</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.112</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12036</th>\n",
       "      <td>100_HingeLoss</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>100_HingeLoss</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016654</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>1.850404</td>\n",
       "      <td>1.735556</td>\n",
       "      <td>#include &lt;torch/extension.h&gt;\\n#include &lt;cuda.h...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\n\\nclass M...</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Op_Name  Level_ID  Task_ID  \\\n",
       "0        1_Square_matrix_multiplication_         1        1   \n",
       "99     2_Standard_matrix_multiplication_         1        2   \n",
       "220      3_Batched_matrix_multiplication         1        3   \n",
       "341      4_Matrix_vector_multiplication_         1        4   \n",
       "462       5_Matrix_scalar_multiplication         1        5   \n",
       "...                                  ...       ...      ...   \n",
       "11152                       96_HuberLoss         1       96   \n",
       "11273            97_CosineSimilarityLoss         1       97   \n",
       "11394                       98_KLDivLoss         1       98   \n",
       "11915               99_TripletMarginLoss         1       99   \n",
       "12036                      100_HingeLoss         1      100   \n",
       "\n",
       "                             Kernel_Name  CUDA_Runtime  \\\n",
       "0        1_Square_matrix_multiplication_         2.115   \n",
       "99     2_Standard_matrix_multiplication_         1.942   \n",
       "220      3_Batched_matrix_multiplication         0.808   \n",
       "341      4_Matrix_vector_multiplication_        18.158   \n",
       "462       5_Matrix_scalar_multiplication         0.229   \n",
       "...                                  ...           ...   \n",
       "11152                       96_HuberLoss         0.012   \n",
       "11273            97_CosineSimilarityLoss         0.014   \n",
       "11394                       98_KLDivLoss         0.014   \n",
       "11915               99_TripletMarginLoss         0.020   \n",
       "12036                      100_HingeLoss         0.009   \n",
       "\n",
       "       PyTorch_Native_Runtime  PyTorch_Compile_Runtime  CUDA_Speedup_Native  \\\n",
       "0                    0.421087                 0.445168             0.199096   \n",
       "99                   0.425295                 0.459274             0.218998   \n",
       "220                  0.128529                 0.181113             0.159070   \n",
       "341                  0.061012                 0.169859             0.003360   \n",
       "462                  0.181897                 0.402290             0.794311   \n",
       "...                       ...                      ...                  ...   \n",
       "11152                0.016157                 0.053737             1.346388   \n",
       "11273                0.076431                 0.054744             5.459388   \n",
       "11394                0.031135                 0.035177             2.223909   \n",
       "11915                0.050352                 0.034053             2.517614   \n",
       "12036                0.016654                 0.015620             1.850404   \n",
       "\n",
       "       CUDA_Speedup_Compile  \\\n",
       "0                  0.210481   \n",
       "99                 0.236495   \n",
       "220                0.224150   \n",
       "341                0.009354   \n",
       "462                1.756726   \n",
       "...                     ...   \n",
       "11152              4.478060   \n",
       "11273              3.910286   \n",
       "11394              2.512667   \n",
       "11915              1.702653   \n",
       "12036              1.735556   \n",
       "\n",
       "                                               CUDA_Code  \\\n",
       "0      #include <torch/extension.h>\\n\\n#include <cuda...   \n",
       "99     #include <torch/extension.h>\\n#include <cuda_r...   \n",
       "220    #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "341    #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "462    #include <torch/extension.h>\\n#include <ATen/c...   \n",
       "...                                                  ...   \n",
       "11152  #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "11273  #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "11394  #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "11915  #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "12036  #include <torch/extension.h>\\n#include <cuda.h...   \n",
       "\n",
       "                                     PyTorch_Code_Module  \\\n",
       "0      import torch\\nimport torch.nn as nn\\n\\n\\nclass...   \n",
       "99     import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "220    import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "341    import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "462    import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "...                                                  ...   \n",
       "11152  import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "11273  import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "11394  import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "11915  import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "12036  import torch\\nimport torch.nn as nn\\n\\nclass M...   \n",
       "\n",
       "                                 PyTorch_Code_Functional  Correct  Max_Diff  \\\n",
       "0      import torch\\nimport torch.nn as nn\\nimport to...     True     0.001   \n",
       "99     import torch\\nimport torch.nn as nn\\nimport to...     True     0.000   \n",
       "220    import torch\\nimport torch.nn as nn\\nimport to...     True     0.000   \n",
       "341    import torch\\nimport torch.nn as nn\\nimport to...     True     0.012   \n",
       "462    import torch\\nimport torch.nn as nn\\nimport to...     True     0.000   \n",
       "...                                                  ...      ...       ...   \n",
       "11152  import torch\\nimport torch.nn as nn\\nimport to...     True     0.000   \n",
       "11273  import torch\\nimport torch.nn as nn\\nimport to...     True     0.000   \n",
       "11394  import torch\\nimport torch.nn as nn\\nimport to...     True     0.007   \n",
       "11915  import torch\\nimport torch.nn as nn\\nimport to...     True     0.112   \n",
       "12036  import torch\\nimport torch.nn as nn\\nimport to...     True     0.103   \n",
       "\n",
       "      Error NCU_Profile Torch_Profile Clang_Tidy  __index_level_0__  \n",
       "0      None        None          None       None                  0  \n",
       "99     None        None          None       None                  0  \n",
       "220    None        None          None       None                  0  \n",
       "341    None        None          None       None                  0  \n",
       "462    None        None          None       None                  0  \n",
       "...     ...         ...           ...        ...                ...  \n",
       "11152  None        None          None       None                  0  \n",
       "11273  None        None          None       None                  0  \n",
       "11394  None        None          None       None                  0  \n",
       "11915  None        None          None       None                  0  \n",
       "12036  None        None          None       None                  0  \n",
       "\n",
       "[91 rows x 19 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sakana Eval Testing (Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"tasks/{df_l1.iloc[0].Op_Name}.py\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].PyTorch_Code_Functional)\n",
    "with open(f\"kernels/{df_l1.iloc[0].Op_Name}.cu\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].CUDA_Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File format is:\n",
    "\n",
    "eval_kernel\n",
    "\n",
    "task/\n",
    "- torch nn module.py\n",
    "- functional.py\n",
    "- info.txt (dont really need this rn)\n",
    "\n",
    "kernel/\n",
    "- kernel.cu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# import os\n",
    "# import torch\n",
    "# import argparse\n",
    "# from torch.utils.cpp_extension import load\n",
    "# from torch.utils._pytree import tree_map\n",
    "# import importlib.util\n",
    "# from torch.utils.benchmark import Timer\n",
    "\n",
    "\n",
    "# def easy_to_device(pytree, device):\n",
    "#     return tree_map(\n",
    "#         lambda x: x.to(device) if isinstance(x, torch.Tensor) else x, pytree\n",
    "#     )\n",
    "\n",
    "\n",
    "# def load_module_from_path(path):\n",
    "#     spec = importlib.util.spec_from_file_location(\"module\", path)\n",
    "#     module = importlib.util.module_from_spec(spec)\n",
    "#     spec.loader.exec_module(module)\n",
    "#     return module\n",
    "\n",
    "\n",
    "# def evaluate(op_name: str):\n",
    "#     # parser = argparse.ArgumentParser()\n",
    "#     # parser.add_argument(\"--op_atol\", type=float, default=1e-3)\n",
    "#     # parser.add_argument(\"--op_rtol\", type=float, default=1e-1)\n",
    "#     # parser.add_argument(\"--rep_time\", type=int, default=10000)\n",
    "#     # parser.add_argument(\"--warmup_time\", type=int, default=25)\n",
    "#     # args = parser.parse_args()\n",
    "\n",
    "#     # # Get task name from info.txt\n",
    "#     # with open(\"task/info.txt\", \"r\") as f:\n",
    "#     #     task_name = f.readline().strip()\n",
    "#     #     task_name = \"_\".join(task_name.split(\"_\")[1:])  # Remove problem ID\n",
    "\n",
    "#     # Import the task module\n",
    "#     # task_files = [f for f in os.listdir(\"tasks\") if f.endswith(\"_functional.py\")]\n",
    "#     # if not task_files:\n",
    "#     #     raise RuntimeError(\"No functional task file found\")\n",
    "\n",
    "#     task = load_module_from_path(os.path.join(\"tasks\", op_name+'.py'))\n",
    "\n",
    "#     # Initialize model and inputs\n",
    "#     device_1 = torch.device(\"cuda:0\")\n",
    "#     torch.manual_seed(0)\n",
    "#     inputs = task.get_inputs()\n",
    "#     init_inputs = task.get_init_inputs()\n",
    "#     model = task.Model(*init_inputs)\n",
    "\n",
    "#     # Load CUDA kernel\n",
    "#     # kernel_files = [f for f in os.listdir(\"kernel\") if f.endswith(\".cu\")]\n",
    "#     # if not kernel_files:\n",
    "#     #     raise RuntimeError(\"No CUDA kernel file found\")\n",
    "    \n",
    "#     task_name = \"_\".join(op_name.split(\"_\")[1:])  # Remove problem ID\n",
    "#     cuda_module = load(\n",
    "#         name=task_name,\n",
    "#         sources=[os.path.join(\"kernels\", op_name+'.cu')],\n",
    "#         extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
    "#         with_cuda=True,\n",
    "#         verbose=True,\n",
    "#     )\n",
    "\n",
    "#     # Test for correctness\n",
    "#     with torch.no_grad():\n",
    "#         cuda_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=cuda_module.forward\n",
    "#         )\n",
    "#         torch_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=task.module_fn\n",
    "#         )\n",
    "\n",
    "#     rtol_default = 1e-1\n",
    "#     atol_default = 1e-3\n",
    "\n",
    "#     correct = torch.allclose(\n",
    "#         torch_output.cpu(),\n",
    "#         cuda_output.cpu(),\n",
    "#         rtol=rtol_default,\n",
    "#         atol=atol_default,\n",
    "#     )\n",
    "#     max_diff = torch.max(torch.abs(torch_output.cpu() - cuda_output.cpu())).item()\n",
    "#     print(f\"Tested CUDA kernel - Correct: {correct}, Max Diff: {max_diff}\")\n",
    "\n",
    "#     if correct:\n",
    "#         # Evaluate CUDA kernel performance\n",
    "#         cuda_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=cuda_module.forward)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"cuda_module\": cuda_module,\n",
    "#             },\n",
    "#         )\n",
    "#         cuda_runtime = cuda_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated CUDA kernel - Runtime: {cuda_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate PyTorch baseline performance\n",
    "#         torch_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=task.module_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"task\": task,\n",
    "#             },\n",
    "#         )\n",
    "#         torch_runtime = torch_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated PyTorch baseline - Runtime: {torch_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate torch compile performance\n",
    "#         torch_fn = task.module_fn\n",
    "#         compile_fn = torch.compile(torch_fn, mode=\"max-autotune\")\n",
    "#         torch_compile_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=compile_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"compile_fn\": compile_fn,\n",
    "#             },\n",
    "#         )\n",
    "\n",
    "#         torch_compile_runtime = torch_compile_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated torch compile - Runtime: {torch_compile_runtime:.3f} ms\")\n",
    "\n",
    "#         print(f\"Speedup over PyTorch: {torch_runtime/cuda_runtime:.2f}x\")\n",
    "#         print(f\"Speedup over torch compile: {torch_compile_runtime/cuda_runtime:.2f}x\")\n",
    "\n",
    "#         import json\n",
    "\n",
    "#         # Store the speedup times as a json file\n",
    "#         file_path = os.path.join(os.path.dirname('speedups'), f\"{op_name}.json\")\n",
    "#         with open(file_path, \"w\") as f:\n",
    "#             json.dump(\n",
    "#                 {\n",
    "#                     \"max_diff\": max_diff,\n",
    "#                     \"cuda_runtime\": cuda_runtime,\n",
    "#                     \"torch_runtime\": torch_runtime,\n",
    "#                     \"torch_compile_runtime\": torch_compile_runtime,\n",
    "#                     \"speedup_over_pytorch\": torch_runtime / cuda_runtime,\n",
    "#                     \"speedup_over_torch_compile\": torch_compile_runtime / cuda_runtime,\n",
    "#                 },\n",
    "#                 f,\n",
    "#             )\n",
    "#         print(f\"Speedup times stored in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(df_l1.iloc[0].Op_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code from KernelBench. Evaluation seems to work with this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhiv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module elementwise_add, skipping build step...\n",
      "Loading extension module elementwise_add...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    # randomly generate input tensors based on the model architecture\n",
    "    a = torch.randn(1, 128).cuda()\n",
    "    b = torch.randn(1, 128).cuda()\n",
    "    return [a, b]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    # randomly generate tensors required for initialization based on the model architecture\n",
    "    return []\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# Define the custom CUDA kernel for element-wise addition\n",
    "elementwise_add_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        out[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "    auto size = a.numel();\n",
    "    auto out = torch::zeros_like(a);\n",
    "\n",
    "    const int block_size = 256;\n",
    "    const int num_blocks = (size + block_size - 1) / block_size;\n",
    "\n",
    "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
    "\n",
    "    return out;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "elementwise_add_cpp_source = (\n",
    "    \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code for element-wise addition\n",
    "elementwise_add = load_inline(\n",
    "    name=\"elementwise_add\",\n",
    "    cpp_sources=elementwise_add_cpp_source,\n",
    "    cuda_sources=elementwise_add_source,\n",
    "    functions=[\"elementwise_add_cuda\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.elementwise_add = elementwise_add\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.elementwise_add.elementwise_add_cuda(a, b)\n",
    "    \n",
    "a, b = get_inputs()\n",
    "torchm = Model()\n",
    "cudam = ModelNew()\n",
    "torch.allclose(torchm.forward(a,b), cudam.forward(a,b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhiv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module matmul_v7, skipping build step...\n",
      "Loading extension module matmul_v7...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-67.5708,  45.0183,  64.8336,  ..., -20.3868, -14.5004, -23.9053],\n",
       "         [-56.4336, -13.6348, -25.7758,  ..., -34.6722,  29.4519,  83.3274],\n",
       "         [ 86.4642, -63.3127,  -0.3111,  ...,  55.4672, -22.2791,   9.9542],\n",
       "         ...,\n",
       "         [-24.4993, -64.4835, -75.8926,  ...,  40.0912, -21.6326,  54.3439],\n",
       "         [ 59.3703, -81.2752, -68.0154,  ...,  -2.8144, -51.7843, -17.1922],\n",
       "         [100.5796,  89.1926,  56.3843,  ..., -55.6670,  40.8864,  61.0896]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-67.5708,  45.0183,  64.8336,  ..., -20.3868, -14.5004, -23.9053],\n",
       "         [-56.4336, -13.6347, -25.7759,  ..., -34.6722,  29.4519,  83.3275],\n",
       "         [ 86.4643, -63.3128,  -0.3111,  ...,  55.4672, -22.2791,   9.9542],\n",
       "         ...,\n",
       "         [-24.4993, -64.4835, -75.8926,  ...,  40.0912, -21.6326,  54.3439],\n",
       "         [ 59.3703, -81.2752, -68.0154,  ...,  -2.8144, -51.7843, -17.1923],\n",
       "         [100.5797,  89.1925,  56.3843,  ..., -55.6670,  40.8864,  61.0896]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single square matrix multiplication (C = A * B).\n",
    "\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
    "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output matrix C of shape (N, N).\n",
    "    \"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
    "        return fn(A, B)\n",
    "\n",
    "\n",
    "N = 2048\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    A = torch.randn(N, N).cuda()\n",
    "    B = torch.randn(N, N).cuda()\n",
    "    return [A, B]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    return []  # No special initialization inputs needed\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code \n",
    "cuda_mod = load_inline(\n",
    "    name=\"matmul\",\n",
    "    cpp_sources=cuda_cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"forward\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cuda_mod = cuda_mod\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.cuda_mod.forward(a, b)\n",
    "\n",
    "# QWEN 7B GENERATED\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# # Define the custom CUDA kernel for matrix multiplication\n",
    "# matmul_kernel_source = \"\"\"\n",
    "# #include <torch/extension.h>\n",
    "# #include <cuda_runtime.h>\n",
    "\n",
    "# __global__ void matmul_kernel(const float* a, const float* b, float* out, int n) {\n",
    "#     int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "#     int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "#     float sum = 0.0f;\n",
    "#     for (int k = 0; k < n; k++) {\n",
    "#         sum += a[row * n + k] * b[k * n + col];\n",
    "#     }\n",
    "#     out[row * n + col] = sum;\n",
    "# }\n",
    "\n",
    "# torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "#     int n = a.size(1);\n",
    "#     int m = b.size(0);\n",
    "#     int k = b.size(1);\n",
    "\n",
    "#     auto out_size = m * k;\n",
    "#     auto out = torch::zeros(m, k, sizeof(float));\n",
    "\n",
    "#     const int block_size = 256;\n",
    "#     const int num_blocks = (out_size + block_size - 1) / block_size;\n",
    "\n",
    "#     matmul_kernel<<<num_blocks, block_size>>>(\n",
    "#         a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), n);\n",
    "\n",
    "#     return out;\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# matmul_cpp_source = (\n",
    "#     \"torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    "# )\n",
    "\n",
    "# # Compile the inline CUDA code for matrix multiplication\n",
    "# matmul = load_inline(\n",
    "#     name=\"matmul\",\n",
    "#     cpp_sources=matmul_cpp_source,\n",
    "#     cuda_sources=matmul_kernel_source,\n",
    "#     functions=[\"matmul_cuda\"],\n",
    "#     verbose=True,\n",
    "#     extra_cflags=[\"-O3\"],\n",
    "#     extra_ldflags=[\"\"],\n",
    "# )\n",
    "\n",
    "# class ModelNew(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.matmul = matmul\n",
    "\n",
    "#     def forward(self, a, b):\n",
    "#         return self.matmul.matmul_cuda(a, b)\n",
    "    \n",
    "torch_mod = Model()\n",
    "cuda_mod = ModelNew()\n",
    "\n",
    "a, b = get_inputs()\n",
    "# a, b, = torch.eye(N).cuda(), torch.eye(N).cuda()\n",
    "print(torch.allclose(torch_mod.forward(a, b), cuda_mod.forward(a, b), rtol=1e-1, atol=1e-3))\n",
    "torch_mod.forward(a, b), cuda_mod.forward(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhiv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "The input conditions for extension module bmm have changed. Bumping to version 1 and re-building as bmm_v1...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/abhiv/.cache/torch_extensions/py310_cu124/bmm/build.ninja...\n",
      "/home/abhiv/miniconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module bmm_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=bmm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/abhiv/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /home/abhiv/.cache/torch_extensions/py310_cu124/bmm/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=bmm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/abhiv/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/abhiv/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /home/abhiv/.cache/torch_extensions/py310_cu124/bmm/cuda.cu -o cuda.cuda.o \n",
      "[3/3] c++ main.o cuda.cuda.o -shared  -L/home/abhiv/miniconda3/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o bmm_v1.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module bmm_v1...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/home/abhiv/.cache/torch_extensions/py310_cu124/bmm/bmm_v1.so: undefined symbol: _Z11forward_bmmN2at6TensorES0_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[231], line 70\u001b[0m\n\u001b[1;32m     65\u001b[0m cuda_cpp_source \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B);\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Compile the inline CUDA code \u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m cuda_mod \u001b[38;5;241m=\u001b[39m \u001b[43mload_inline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbmm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpp_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_cpp_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcuda_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward_bmm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelNew\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1723\u001b[0m, in \u001b[0;36mload_inline\u001b[0;34m(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates, use_pch)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     _maybe_write(cuda_source_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cuda_sources))\n\u001b[1;32m   1721\u001b[0m     sources\u001b[38;5;241m.\u001b[39mappend(cuda_source_path)\n\u001b[0;32m-> 1723\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1823\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_standalone:\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_exec_path(name, build_directory)\n\u001b[0;32m-> 1823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_module_from_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2245\u001b[0m, in \u001b[0;36m_import_module_from_library\u001b[0;34m(module_name, path, is_python_module)\u001b[0m\n\u001b[1;32m   2243\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(module_name, filepath)\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2245\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_from_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, importlib\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[1;32m   2247\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:571\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1176\u001b[0m, in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/abhiv/.cache/torch_extensions/py310_cu124/bmm/bmm_v1.so: undefined symbol: _Z11forward_bmmN2at6TensorES0_"
     ]
    }
   ],
   "source": [
    "exec(l1_samples.iloc[2].PyTorch_Code_Functional)\n",
    "cuda_src = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel for batched matrix multiplication: C = A * B\n",
    "// Shapes: A (batch_size, M, K), B (batch_size, K, N), C (batch_size, M, N)\n",
    "__global__ void bmm_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    float* __restrict__ C,\n",
    "    int batch_size,\n",
    "    int M,\n",
    "    int K,\n",
    "    int N\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * M * N;\n",
    "    if (idx >= total) return;\n",
    "\n",
    "    int b = idx / (M * N);\n",
    "    int remainder = idx % (M * N);\n",
    "    int m = remainder / N;\n",
    "    int n = remainder % N;\n",
    "\n",
    "    float val = 0.0f;\n",
    "    for (int k = 0; k < K; k++) {\n",
    "        val += A[b * M * K + m * K + k] * B[b * K * N + k * N + n];\n",
    "    }\n",
    "    C[b * M * N + m * N + n] = val;\n",
    "}\n",
    "\n",
    "torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B) {\n",
    "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
    "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
    "    TORCH_CHECK(A.dim() == 3, \"A must be 3D\");\n",
    "    TORCH_CHECK(B.dim() == 3, \"B must be 3D\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"Batch sizes must match\");\n",
    "    TORCH_CHECK(A.size(2) == B.size(1), \"Inner dimensions (K) must match\");\n",
    "\n",
    "    int batch_size = A.size(0);\n",
    "    int M = A.size(1);\n",
    "    int K = A.size(2);\n",
    "    int N = B.size(2);\n",
    "\n",
    "    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n",
    "    auto C = torch::zeros({batch_size, M, N}, options);\n",
    "\n",
    "    int total = batch_size * M * N;\n",
    "    const int threads = 256;\n",
    "    int blocks = (total + threads - 1) / threads;\n",
    "\n",
    "    bmm_kernel<<<blocks, threads>>>(\n",
    "        A.data_ptr<float>(),\n",
    "        B.data_ptr<float>(),\n",
    "        C.data_ptr<float>(),\n",
    "        batch_size, M, K, N\n",
    "    );\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_cpp_source = (\n",
    "    \"torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code \n",
    "cuda_mod = load_inline(\n",
    "    name=\"bmm\",\n",
    "    cpp_sources=cuda_cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"forward_bmm\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cuda_mod = cuda_mod\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.cuda_mod.forward_bmm(a, b)\n",
    "\n",
    "a, b = get_inputs()\n",
    "a = a.cuda()\n",
    "b = b.cuda()\n",
    "print(torch.allclose(torch_mod.forward(a, b), cuda_mod.forward(a, b), rtol=1e-1, atol=1e-3))\n",
    "torch_mod.forward(a, b), cuda_mod.forward(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- ~~evaluation stuff~~\n",
    "- prompting qwen for good outputs \n",
    "- ~~KernelBench method - NA~~\n",
    "- coding the RL portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <torch/extension.h>\n",
      "#include <cuda.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "// CUDA kernel for batched matrix multiplication: C = A * B\n",
      "// Shapes: A (batch_size, M, K), B (batch_size, K, N), C (batch_size, M, N)\n",
      "__global__ void bmm_kernel(\n",
      "    const float* __restrict__ A,\n",
      "    const float* __restrict__ B,\n",
      "    float* __restrict__ C,\n",
      "    int batch_size,\n",
      "    int M,\n",
      "    int K,\n",
      "    int N\n",
      ") {\n",
      "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int total = batch_size * M * N;\n",
      "    if (idx >= total) return;\n",
      "\n",
      "    int b = idx / (M * N);\n",
      "    int remainder = idx % (M * N);\n",
      "    int m = remainder / N;\n",
      "    int n = remainder % N;\n",
      "\n",
      "    float val = 0.0f;\n",
      "    for (int k = 0; k < K; k++) {\n",
      "        val += A[b * M * K + m * K + k] * B[b * K * N + k * N + n];\n",
      "    }\n",
      "    C[b * M * N + m * N + n] = val;\n",
      "}\n",
      "\n",
      "torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B) {\n",
      "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
      "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
      "    TORCH_CHECK(A.dim() == 3, \"A must be 3D\");\n",
      "    TORCH_CHECK(B.dim() == 3, \"B must be 3D\");\n",
      "    TORCH_CHECK(A.size(0) == B.size(0), \"Batch sizes must match\");\n",
      "    TORCH_CHECK(A.size(2) == B.size(1), \"Inner dimensions (K) must match\");\n",
      "\n",
      "    int batch_size = A.size(0);\n",
      "    int M = A.size(1);\n",
      "    int K = A.size(2);\n",
      "    int N = B.size(2);\n",
      "\n",
      "    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n",
      "    auto C = torch::zeros({batch_size, M, N}, options);\n",
      "\n",
      "    int total = batch_size * M * N;\n",
      "    const int threads = 256;\n",
      "    int blocks = (total + threads - 1) / threads;\n",
      "\n",
      "    bmm_kernel<<<blocks, threads>>>(\n",
      "        A.data_ptr<float>(),\n",
      "        B.data_ptr<float>(),\n",
      "        C.data_ptr<float>(),\n",
      "        batch_size, M, K, N\n",
      "    );\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\n",
      "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
      "    m.def(\"forward\", &forward_bmm, \"Batched matrix multiplication (CUDA)\");\n",
      "}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
