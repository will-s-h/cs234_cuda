{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhiv/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    level_1: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12157\n",
      "    })\n",
      "    level_2: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12938\n",
      "    })\n",
      "    level_3: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 5520\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the dataset name and the cache directory\n",
    "dataset_name = \"SakanaAI/AI-CUDA-Engineer-Archive\"\n",
    "cache_dir = \"./cache_dir\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "\n",
    "# Print the dataset to verify\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = dataset[\"level_1\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <torch/extension.h>\n",
      "\n",
      "#include <cuda.h>\n",
      "#include <cuda_runtime.h>\n",
      "#include <c10/cuda/CUDAException.h>\n",
      "\n",
      "#define TILE_SIZE 16\n",
      "\n",
      "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
      "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
      "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
      "\n",
      "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
      "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
      "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
      "\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "\n",
      "    int row = blockIdx.y * TILE_SIZE + ty;\n",
      "    int col = blockIdx.x * TILE_SIZE + tx;\n",
      "\n",
      "    float C_value = 0.0f;\n",
      "\n",
      "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
      "        // Load tiles into shared memory\n",
      "        if (row < N && m * TILE_SIZE + tx < N)\n",
      "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
      "        else\n",
      "            As[ty][tx] = 0.0f;\n",
      "\n",
      "        if (col < N && m * TILE_SIZE + ty < N)\n",
      "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
      "        else\n",
      "            Bs[ty][tx] = 0.0f;\n",
      "\n",
      "        __syncthreads();\n",
      "\n",
      "        // Compute partial product\n",
      "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
      "            C_value += As[ty][k] * Bs[k][tx];\n",
      "        }\n",
      "\n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    // Write the result\n",
      "    if (row < N && col < N)\n",
      "        C[row * N + col] = C_value;\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    CHECK_INPUT(A);\n",
      "    CHECK_INPUT(B);\n",
      "    CHECK_FLOAT(A);\n",
      "    CHECK_FLOAT(B);\n",
      "\n",
      "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
      "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
      "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
      "\n",
      "    int64_t N = A.size(0);\n",
      "\n",
      "    auto C = torch::zeros({N, N}, A.options());\n",
      "\n",
      "    const float* A_data = A.data_ptr<float>();\n",
      "    const float* B_data = B.data_ptr<float>();\n",
      "    float* C_data = C.data_ptr<float>();\n",
      "\n",
      "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
      "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
      "\n",
      "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
      "\n",
      "    // Check for kernel launch errors\n",
      "    C10_CUDA_CHECK(cudaGetLastError());\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\n",
      "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
      "    m.def(\"forward\", &forward, \"Matrix multiplication kernel (CUDA)\");\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(df_l1.iloc[0][[\"CUDA_Code\"]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_l1.iloc[0][[\"PyTorch_Code_Functional\"]].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_q14 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "model_q7 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "# model_q1 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_q7,torch_dtype=\"auto\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_q7, torch_dtype=\"auto\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "with torch.cuda.device(0):  # explicitly set GPU 0 if needed\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "# tokenizer = tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Decode the generated tokens to get the response\u001b[39;00m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:275\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    274\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:222\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    220\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    221\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 222\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"what is the solution of x^2 - 2x + 1 = 0?<think>\"\n",
    "\n",
    "# prompt = \"what is the second planet from the Sun?<think>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_length=10_000)\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = inputs.to('cpu')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n\\ndef module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs a single square matrix multiplication (C = A * B).\\n\\n    Args:\\n        A (torch.Tensor): Input matrix A of shape (N, N).\\n        B (torch.Tensor): Input matrix B of shape (N, N).\\n\\n    Returns:\\n        torch.Tensor: Output matrix C of shape (N, N).\\n    \"\"\"\\n    return torch.matmul(A, B)\\n\\n\\nclass Model(nn.Module):\\n    \"\"\"\\n    Simple model that performs a single square matrix multiplication (C = A * B)\\n    \"\"\"\\n\\n    def __init__(self):\\n        super(Model, self).__init__()\\n\\n    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\\n        return fn(A, B)\\n\\n\\nN = 2048\\n\\n\\ndef get_inputs():\\n    A = torch.randn(N, N)\\n    B = torch.randn(N, N)\\n    return [A, B]\\n\\n\\ndef get_init_inputs():\\n    return []  # No special initialization inputs needed\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_l1.iloc[0].PyTorch_Code_Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# CUDA Prompt\n",
    "############################################\n",
    "# PROBLEM_STATEMENT = \"\"\"You write custom CUDA kernels to replace the pytorch operators in the given architecture to get speedups. \\n\n",
    "#     You have complete freedom to choose the set of operators you want to replace. You may make the decision to replace some operators with custom CUDA kernels and leave others unchanged. You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\\n\n",
    "# \"\"\"\n",
    "# PROBLEM_INSTRUCTION = \"\"\"\n",
    "# Optimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \\n\n",
    "# \"\"\"\n",
    "PROBLEM_STATEMENT = \"\"\"You write custom CUDA kernels to replace the pytorch operators in the given architecture to get speedups. \\n\n",
    "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\\n\n",
    "\"\"\"\n",
    "PROBLEM_INSTRUCTION = \"\"\"\n",
    "Optimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt_generate_custom_cuda(\n",
    "    arc_src: str, example_arch_src: str, example_new_arch_src: str\n",
    ") -> str:\n",
    "    prompt = PROBLEM_STATEMENT\n",
    "\n",
    "    if example_arch_src != \"\" and example_new_arch_src != \"\":\n",
    "        prompt += f\"\"\"\n",
    "        Here's an example to show you the syntax of the architecture you will see implemented in torch: The example architecture is for element-wise addition: \\n\n",
    "        ``` \\n\n",
    "        {example_arch_src}\n",
    "        ``` \\n\n",
    "        The example new arch with custom CUDA kernels looks like this: \n",
    "        ```\n",
    "        {example_new_arch_src}\n",
    "        ``` \\n\n",
    "        \"\"\"\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "    You are given the following torch architecture: \\n\n",
    "    ```\n",
    "    {arc_src}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    prompt += PROBLEM_INSTRUCTION\n",
    "    return prompt\n",
    "\n",
    "def prompt_generate_custom_cuda_from_prompt_template(ref_arch_src: str) -> str:\n",
    "    \"\"\"\n",
    "    Using prompt example (an element-wise addition) for prompt templates\n",
    "    The most basic form of example just to show LLM the task and the expected output format\n",
    "    \"\"\"\n",
    "    arch = ref_arch_src\n",
    "    # These are strictly defined for now\n",
    "\n",
    "    # # path to prompt template, show an example of Model (torch specifications) and ModelNew (torch + custom CUDA kernels)\n",
    "    # example_arch_path = os.path.join(\n",
    "    #     REPO_TOP_PATH, f\"src/prompts/model_ex_add.py\"\n",
    "    # )\n",
    "    # example_new_arch_path = os.path.join(\n",
    "    #     REPO_TOP_PATH, f\"src/prompts/model_new_ex_add.py\"\n",
    "    # )\n",
    "\n",
    "    # if not os.path.exists(example_arch_path):\n",
    "    #     raise FileNotFoundError(\n",
    "    #         f\"Example architecture file not found: {example_arch_path}\"\n",
    "    #     )\n",
    "    # if not os.path.exists(example_new_arch_path):\n",
    "    #     raise FileNotFoundError(\n",
    "    #         f\"Example new architecture file not found: {example_new_arch_path}\"\n",
    "    #     )\n",
    "\n",
    "    example_arch = '''\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, a, b):\n",
    "            return a + b\n",
    "\n",
    "\n",
    "    def get_inputs():\n",
    "        # randomly generate input tensors based on the model architecture\n",
    "        a = torch.randn(1, 128).cuda()\n",
    "        b = torch.randn(1, 128).cuda()\n",
    "        return [a, b]\n",
    "\n",
    "\n",
    "    def get_init_inputs():\n",
    "        # randomly generate tensors required for initialization based on the model architecture\n",
    "        return []\n",
    "'''\n",
    "    example_new_arch = '''\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "    # Define the custom CUDA kernel for element-wise addition\n",
    "    elementwise_add_source = \"\"\"\n",
    "    #include <torch/extension.h>\n",
    "    #include <cuda_runtime.h>\n",
    "\n",
    "    __global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
    "        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (idx < size) {\n",
    "            out[idx] = a[idx] + b[idx];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "        auto size = a.numel();\n",
    "        auto out = torch::zeros_like(a);\n",
    "\n",
    "        const int block_size = 256;\n",
    "        const int num_blocks = (size + block_size - 1) / block_size;\n",
    "\n",
    "        elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
    "\n",
    "        return out;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    elementwise_add_cpp_source = (\n",
    "        \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    "    )\n",
    "\n",
    "    # Compile the inline CUDA code for element-wise addition\n",
    "    try:\n",
    "        elementwise_add = load_inline(\n",
    "            name=\"elementwise_add\",\n",
    "            cpp_sources=elementwise_add_cpp_source,\n",
    "            cuda_sources=elementwise_add_source,\n",
    "            functions=[\"elementwise_add_cuda\"],\n",
    "            verbose=True,\n",
    "            extra_cflags=[\"-O3\"],\n",
    "            extra_ldflags=[\"\"],\n",
    "        )\n",
    "        \n",
    "        class ModelNew(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.elementwise_add = elementwise_add\n",
    "\n",
    "        def forward(self, a, b):\n",
    "            return self.elementwise_add.elementwise_add_cuda(a, b)\n",
    "    except:\n",
    "        print(\"CUDA Kernel had error loading in. Probably an error in the outputted code\")\n",
    "        continue\n",
    "\n",
    "'''\n",
    "\n",
    "    return prompt_generate_custom_cuda(arch, example_arch, example_new_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "functional_str = df_l1.iloc[0].PyTorch_Code_Functional\n",
    "prompt = prompt_generate_custom_cuda_from_prompt_template(functional_str)\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "# Tokenize the input promptb\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=7_000)\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# inputs = inputs.to('cpu')\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(functional_str[functional_str.find('def module_fn'):functional_str.find('class Model')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.cpp_extension import load_inline\n",
      "\n",
      "# Define the custom CUDA kernel for matrix multiplication\n",
      "matmul_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
      "    int i = blockIdx_row;\n",
      "    int j = blockIdx_col;\n",
      "    int k = threadIdx_col;\n",
      "\n",
      "    __shared__ float A_block[N/4];\n",
      "    __shared__ float B_block[N/4];\n",
      "    \n",
      "    float sum = 0.0f;\n",
      "    for (int t = 0; t < N/4; ++t) {\n",
      "        A_block[t] = A[t * N + i];\n",
      "        B_block[t] = B[t * N + j];\n",
      "        for (int n = 0; n < N/4; ++n) {\n",
      "            sum += A_block[t] * B_block[n];\n",
      "        }\n",
      "        C[i * N + j] = sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n",
      "    int N = A.size(1);\n",
      "    auto C = torch::empty(N, N, A.dtype());\n",
      "    const int block_size = 256;\n",
      "    \n",
      "    const int num_blocks = (N + block_size - 1) / block_size;\n",
      "    \n",
      "    matmul_kernel<<<num_blocks, block_size>>>(A.data_ptr(), B.data_ptr(), C.data_ptr(), N);\n",
      "    return C;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "matmulcpp_source = \"\"\"\n",
      "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n",
      "    int N = A.size(1);\n",
      "    auto C = torch::empty(N, N, A.dtype());\n",
      "    const int block_size = 256;\n",
      "    \n",
      "    const int num_blocks = (N + block_size - 1) / block_size;\n",
      "    \n",
      "    matmul_kernel<<<num_blocks, block_size>>>(A.data_ptr(), B.data_ptr(), C.data_ptr(), N);\n",
      "    return C;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "elementwise_add_cpp_source = (\n",
      "    \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
      ")\n",
      "\n",
      "# Compile the inline CUDA code for matrix multiplication\n",
      "try:\n",
      "    matmul = load_inline(\n",
      "        name=\"matmul\",\n",
      "        cpp_sources=elementwise_add_cpp_source,\n",
      "        cuda_sources=matmul_source,\n",
      "        functions=[\"matmul_cuda\"],\n",
      "        verbose=True,\n",
      "        extra_cflags=[\"-O3\"],\n",
      "        extra_ldflags=[\"\"],\n",
      "    )\n",
      "\n",
      "except:\n",
      "    print(\"CUDA Kernel had error loading in. Probably an error in the outputted code\")\n",
      "\n",
      "class ModelNew(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    Using custom CUDA kernel for matrix multiplication\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(ModelNew, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "        return matmul(A, B)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(response[response.find(\"</think>\")+len(\"</think>\"):])\n",
    "output = response[response.find(\"</think>\")+len(\"</think>\"):]\n",
    "output = output[output.find('```')+4:output.find('```', 10)]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_samples = df_l1[df_l1.Kernel_Name == df_l1.Op_Name]\n",
    "for func_str in l1_samples.PyTorch_Code_Functional.iloc[:4]:\n",
    "    prompt = prompt_generate_custom_cuda_from_prompt_template(func_str)\n",
    "    # Tokenize the input promptb\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # Generate the model's response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10_000)\n",
    "    # Decode the generated tokens to get the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    output = response[response.find(\"</think>\")+len(\"</think>\"):]\n",
    "    print(func_str, output)\n",
    "    output = output[output.find('```')+4:output.find('```', 10)]\n",
    "    print(output)\n",
    "\n",
    "    exec(func_str)\n",
    "    inputs = get_inputs()\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].cuda()\n",
    "    exec()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sakana Eval Testing (Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"tasks/{df_l1.iloc[0].Op_Name}.py\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].PyTorch_Code_Functional)\n",
    "with open(f\"kernels/{df_l1.iloc[0].Op_Name}.cu\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].CUDA_Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File format is:\n",
    "\n",
    "eval_kernel\n",
    "\n",
    "task/\n",
    "- torch nn module.py\n",
    "- functional.py\n",
    "- info.txt (dont really need this rn)\n",
    "\n",
    "kernel/\n",
    "- kernel.cu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# import os\n",
    "# import torch\n",
    "# import argparse\n",
    "# from torch.utils.cpp_extension import load\n",
    "# from torch.utils._pytree import tree_map\n",
    "# import importlib.util\n",
    "# from torch.utils.benchmark import Timer\n",
    "\n",
    "\n",
    "# def easy_to_device(pytree, device):\n",
    "#     return tree_map(\n",
    "#         lambda x: x.to(device) if isinstance(x, torch.Tensor) else x, pytree\n",
    "#     )\n",
    "\n",
    "\n",
    "# def load_module_from_path(path):\n",
    "#     spec = importlib.util.spec_from_file_location(\"module\", path)\n",
    "#     module = importlib.util.module_from_spec(spec)\n",
    "#     spec.loader.exec_module(module)\n",
    "#     return module\n",
    "\n",
    "\n",
    "# def evaluate(op_name: str):\n",
    "#     # parser = argparse.ArgumentParser()\n",
    "#     # parser.add_argument(\"--op_atol\", type=float, default=1e-3)\n",
    "#     # parser.add_argument(\"--op_rtol\", type=float, default=1e-1)\n",
    "#     # parser.add_argument(\"--rep_time\", type=int, default=10000)\n",
    "#     # parser.add_argument(\"--warmup_time\", type=int, default=25)\n",
    "#     # args = parser.parse_args()\n",
    "\n",
    "#     # # Get task name from info.txt\n",
    "#     # with open(\"task/info.txt\", \"r\") as f:\n",
    "#     #     task_name = f.readline().strip()\n",
    "#     #     task_name = \"_\".join(task_name.split(\"_\")[1:])  # Remove problem ID\n",
    "\n",
    "#     # Import the task module\n",
    "#     # task_files = [f for f in os.listdir(\"tasks\") if f.endswith(\"_functional.py\")]\n",
    "#     # if not task_files:\n",
    "#     #     raise RuntimeError(\"No functional task file found\")\n",
    "\n",
    "#     task = load_module_from_path(os.path.join(\"tasks\", op_name+'.py'))\n",
    "\n",
    "#     # Initialize model and inputs\n",
    "#     device_1 = torch.device(\"cuda:0\")\n",
    "#     torch.manual_seed(0)\n",
    "#     inputs = task.get_inputs()\n",
    "#     init_inputs = task.get_init_inputs()\n",
    "#     model = task.Model(*init_inputs)\n",
    "\n",
    "#     # Load CUDA kernel\n",
    "#     # kernel_files = [f for f in os.listdir(\"kernel\") if f.endswith(\".cu\")]\n",
    "#     # if not kernel_files:\n",
    "#     #     raise RuntimeError(\"No CUDA kernel file found\")\n",
    "    \n",
    "#     task_name = \"_\".join(op_name.split(\"_\")[1:])  # Remove problem ID\n",
    "#     cuda_module = load(\n",
    "#         name=task_name,\n",
    "#         sources=[os.path.join(\"kernels\", op_name+'.cu')],\n",
    "#         extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
    "#         with_cuda=True,\n",
    "#         verbose=True,\n",
    "#     )\n",
    "\n",
    "#     # Test for correctness\n",
    "#     with torch.no_grad():\n",
    "#         cuda_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=cuda_module.forward\n",
    "#         )\n",
    "#         torch_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=task.module_fn\n",
    "#         )\n",
    "\n",
    "#     rtol_default = 1e-1\n",
    "#     atol_default = 1e-3\n",
    "\n",
    "#     correct = torch.allclose(\n",
    "#         torch_output.cpu(),\n",
    "#         cuda_output.cpu(),\n",
    "#         rtol=rtol_default,\n",
    "#         atol=atol_default,\n",
    "#     )\n",
    "#     max_diff = torch.max(torch.abs(torch_output.cpu() - cuda_output.cpu())).item()\n",
    "#     print(f\"Tested CUDA kernel - Correct: {correct}, Max Diff: {max_diff}\")\n",
    "\n",
    "#     if correct:\n",
    "#         # Evaluate CUDA kernel performance\n",
    "#         cuda_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=cuda_module.forward)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"cuda_module\": cuda_module,\n",
    "#             },\n",
    "#         )\n",
    "#         cuda_runtime = cuda_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated CUDA kernel - Runtime: {cuda_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate PyTorch baseline performance\n",
    "#         torch_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=task.module_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"task\": task,\n",
    "#             },\n",
    "#         )\n",
    "#         torch_runtime = torch_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated PyTorch baseline - Runtime: {torch_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate torch compile performance\n",
    "#         torch_fn = task.module_fn\n",
    "#         compile_fn = torch.compile(torch_fn, mode=\"max-autotune\")\n",
    "#         torch_compile_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=compile_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"compile_fn\": compile_fn,\n",
    "#             },\n",
    "#         )\n",
    "\n",
    "#         torch_compile_runtime = torch_compile_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated torch compile - Runtime: {torch_compile_runtime:.3f} ms\")\n",
    "\n",
    "#         print(f\"Speedup over PyTorch: {torch_runtime/cuda_runtime:.2f}x\")\n",
    "#         print(f\"Speedup over torch compile: {torch_compile_runtime/cuda_runtime:.2f}x\")\n",
    "\n",
    "#         import json\n",
    "\n",
    "#         # Store the speedup times as a json file\n",
    "#         file_path = os.path.join(os.path.dirname('speedups'), f\"{op_name}.json\")\n",
    "#         with open(file_path, \"w\") as f:\n",
    "#             json.dump(\n",
    "#                 {\n",
    "#                     \"max_diff\": max_diff,\n",
    "#                     \"cuda_runtime\": cuda_runtime,\n",
    "#                     \"torch_runtime\": torch_runtime,\n",
    "#                     \"torch_compile_runtime\": torch_compile_runtime,\n",
    "#                     \"speedup_over_pytorch\": torch_runtime / cuda_runtime,\n",
    "#                     \"speedup_over_torch_compile\": torch_compile_runtime / cuda_runtime,\n",
    "#                 },\n",
    "#                 f,\n",
    "#             )\n",
    "#         print(f\"Speedup times stored in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(df_l1.iloc[0].Op_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code from KernelBench. Evaluation seems to work with this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhiv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module elementwise_add, skipping build step...\n",
      "Loading extension module elementwise_add...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    # randomly generate input tensors based on the model architecture\n",
    "    a = torch.randn(1, 128).cuda()\n",
    "    b = torch.randn(1, 128).cuda()\n",
    "    return [a, b]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    # randomly generate tensors required for initialization based on the model architecture\n",
    "    return []\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# Define the custom CUDA kernel for element-wise addition\n",
    "elementwise_add_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        out[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "    auto size = a.numel();\n",
    "    auto out = torch::zeros_like(a);\n",
    "\n",
    "    const int block_size = 256;\n",
    "    const int num_blocks = (size + block_size - 1) / block_size;\n",
    "\n",
    "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
    "\n",
    "    return out;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "elementwise_add_cpp_source = (\n",
    "    \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code for element-wise addition\n",
    "elementwise_add = load_inline(\n",
    "    name=\"elementwise_add\",\n",
    "    cpp_sources=elementwise_add_cpp_source,\n",
    "    cuda_sources=elementwise_add_source,\n",
    "    functions=[\"elementwise_add_cuda\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.elementwise_add = elementwise_add\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.elementwise_add.elementwise_add_cuda(a, b)\n",
    "    \n",
    "a, b = get_inputs()\n",
    "torchm = Model()\n",
    "cudam = ModelNew()\n",
    "torch.allclose(torchm.forward(a,b), cudam.forward(a,b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Kernel had error loading in. Probably an error in the outputted code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhiv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module matrix_mult_v1, skipping build step...\n",
      "Loading extension module matrix_mult_v1...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single square matrix multiplication (C = A * B).\n",
    "\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
    "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output matrix C of shape (N, N).\n",
    "    \"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
    "        return fn(A, B)\n",
    "\n",
    "\n",
    "N = 2048\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    A = torch.randn(N, N).cuda()\n",
    "    B = torch.randn(N, N).cuda()\n",
    "    return [A, B]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    return []  # No special initialization inputs needed\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# cuda_source = \"\"\"\n",
    "# #include <torch/extension.h>\n",
    "\n",
    "# #include <cuda.h>\n",
    "# #include <cuda_runtime.h>\n",
    "# #include <c10/cuda/CUDAException.h>\n",
    "\n",
    "# #define TILE_SIZE 16\n",
    "\n",
    "# #define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "# #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "# #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "# #define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "# __global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "#     __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "#     __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "#     int tx = threadIdx.x;\n",
    "#     int ty = threadIdx.y;\n",
    "\n",
    "#     int row = blockIdx.y * TILE_SIZE + ty;\n",
    "#     int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "#     float C_value = 0.0f;\n",
    "\n",
    "#     for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "#         // Load tiles into shared memory\n",
    "#         if (row < N && m * TILE_SIZE + tx < N)\n",
    "#             As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "#         else\n",
    "#             As[ty][tx] = 0.0f;\n",
    "\n",
    "#         if (col < N && m * TILE_SIZE + ty < N)\n",
    "#             Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "#         else\n",
    "#             Bs[ty][tx] = 0.0f;\n",
    "\n",
    "#         __syncthreads();\n",
    "\n",
    "#         // Compute partial product\n",
    "#         for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "#             C_value += As[ty][k] * Bs[k][tx];\n",
    "#         }\n",
    "\n",
    "#         __syncthreads();\n",
    "#     }\n",
    "\n",
    "#     // Write the result\n",
    "#     if (row < N && col < N)\n",
    "#         C[row * N + col] = C_value;\n",
    "# }\n",
    "\n",
    "# torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "#     CHECK_INPUT(A);\n",
    "#     CHECK_INPUT(B);\n",
    "#     CHECK_FLOAT(A);\n",
    "#     CHECK_FLOAT(B);\n",
    "\n",
    "#     TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "#     TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "#     TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "#     int64_t N = A.size(0);\n",
    "\n",
    "#     auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "#     const float* A_data = A.data_ptr<float>();\n",
    "#     const float* B_data = B.data_ptr<float>();\n",
    "#     float* C_data = C.data_ptr<float>();\n",
    "\n",
    "#     dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "#     dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "#     matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "#     // Check for kernel launch errors\n",
    "#     C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "#     return C;\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# cuda_cpp_source = (\n",
    "#     \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    "# )\n",
    "\n",
    "# # Compile the inline CUDA code \n",
    "# cuda_mod = load_inline(\n",
    "#     name=\"matmul\",\n",
    "#     cpp_sources=cuda_cpp_source,\n",
    "#     cuda_sources=cuda_source,\n",
    "#     functions=[\"forward\"],\n",
    "#     verbose=True,\n",
    "#     extra_cflags=[\"\"],\n",
    "#     extra_ldflags=[\"\"],\n",
    "# )\n",
    "\n",
    "\n",
    "# class ModelNew(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.cuda_mod = cuda_mod\n",
    "\n",
    "#     def forward(self, a, b):\n",
    "#         return self.cuda_mod.forward(a, b)\n",
    "\n",
    "# QWEN 7B GENERATED\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# Define the custom CUDA kernel for matrix multiplication\n",
    "matrix_mult_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matrix_mult_kernel(const float* A, const float* B, float* C, int n) {\n",
    "    int row = blockIdx.y;\n",
    "    int col = blockIdx.x;\n",
    "    int tx = threadIdx.x;\n",
    "\n",
    "    float C_val = 0.0f;\n",
    "    for (int k = 0; k < n; ++k) {\n",
    "        __syncthreads();\n",
    "        float A_val = A[row * n + k];\n",
    "        float B_val = B[col * n + k];\n",
    "        C_val += A_val * B_val;\n",
    "    }\n",
    "\n",
    "    C[col * n + row] = C_val;\n",
    "}\n",
    "\n",
    "torch::Tensor matrix_mult_cpu(torch::Tensor A, torch::Tensor B) {\n",
    "    int n = A.size(1);\n",
    "    auto C = torch::zeros(A.size(0), B.size(0));\n",
    "    grid = griddim(n);\n",
    "    \n",
    "    matrix_mult_kernel<<<grid, griddim>>>(A.data_ptr(), B.data_ptr(), C.data_ptr(), n);\n",
    "    \n",
    "    return C;\n",
    "}\n",
    "\n",
    "// Grid configuration for the kernel\n",
    "#define BLOCK_SIZE 256\n",
    "#define GRID_SIZE (N + BLOCK_SIZE - 1) / BLOCK_SIZE\n",
    "\"\"\"\n",
    "\n",
    "matrix_mult_cpp_source = (\n",
    "    \"torch::Tensor matrix_mult_cuda(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the custom CUDA code for matrix multiplication\n",
    "try:\n",
    "    matrix_mult = load_inline(\n",
    "        name=\"matrix_mult\",\n",
    "        cpp_sources=matrix_mult_cpp_source,\n",
    "        cuda_sources=matrix_mult_source,\n",
    "        functions=[\"matrix_mult_cuda\"],\n",
    "        verbose=True,\n",
    "        extra_cflags=[\"-O3\"],\n",
    "        extra_ldflags=[\"\"],\n",
    "    )\n",
    "except:\n",
    "    print(\"CUDA Kernel had error loading in. Probably an error in the outputted code\")\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "        return matrix_mult(A, B)\n",
    "    \n",
    "torch_mod = Model()\n",
    "cuda_mod = ModelNew()\n",
    "\n",
    "a, b = get_inputs()\n",
    "# a, b, = torch.eye(N).cuda(), torch.eye(N).cuda()\n",
    "# torch.allclose(torch_mod.forward(a, b), cuda_mod.forward(a, b), rtol=1e-1, atol=1e-3)\n",
    "# torch_mod.forward(a, b), cuda_mod.forward(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- ~~evaluation stuff~~\n",
    "- prompting qwen for good outputs \n",
    "- ~~KernelBench method - NA~~\n",
    "- coding the RL portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
