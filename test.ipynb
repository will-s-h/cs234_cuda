{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DISK/conda_envs/cs234/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    level_1: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12157\n",
      "    })\n",
      "    level_2: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12938\n",
      "    })\n",
      "    level_3: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 5520\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "# Specify the dataset name and the cache directory\n",
    "dataset_name = \"SakanaAI/AI-CUDA-Engineer-Archive\"\n",
    "cache_dir = \"./cache_dir\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ['TORCH_EXTENSIONS_DIR'] = cache_dir\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "\n",
    "# Print the dataset to verify\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = dataset[\"level_1\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <torch/extension.h>\n",
      "\n",
      "#include <cuda.h>\n",
      "#include <cuda_runtime.h>\n",
      "#include <c10/cuda/CUDAException.h>\n",
      "\n",
      "#define TILE_SIZE 16\n",
      "\n",
      "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
      "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
      "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
      "\n",
      "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
      "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
      "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
      "\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "\n",
      "    int row = blockIdx.y * TILE_SIZE + ty;\n",
      "    int col = blockIdx.x * TILE_SIZE + tx;\n",
      "\n",
      "    float C_value = 0.0f;\n",
      "\n",
      "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
      "        // Load tiles into shared memory\n",
      "        if (row < N && m * TILE_SIZE + tx < N)\n",
      "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
      "        else\n",
      "            As[ty][tx] = 0.0f;\n",
      "\n",
      "        if (col < N && m * TILE_SIZE + ty < N)\n",
      "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
      "        else\n",
      "            Bs[ty][tx] = 0.0f;\n",
      "\n",
      "        __syncthreads();\n",
      "\n",
      "        // Compute partial product\n",
      "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
      "            C_value += As[ty][k] * Bs[k][tx];\n",
      "        }\n",
      "\n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    // Write the result\n",
      "    if (row < N && col < N)\n",
      "        C[row * N + col] = C_value;\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    CHECK_INPUT(A);\n",
      "    CHECK_INPUT(B);\n",
      "    CHECK_FLOAT(A);\n",
      "    CHECK_FLOAT(B);\n",
      "\n",
      "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
      "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
      "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
      "\n",
      "    int64_t N = A.size(0);\n",
      "\n",
      "    auto C = torch::zeros({N, N}, A.options());\n",
      "\n",
      "    const float* A_data = A.data_ptr<float>();\n",
      "    const float* B_data = B.data_ptr<float>();\n",
      "    float* C_data = C.data_ptr<float>();\n",
      "\n",
      "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
      "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
      "\n",
      "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
      "\n",
      "    // Check for kernel launch errors\n",
      "    C10_CUDA_CHECK(cudaGetLastError());\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\n",
      "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
      "    m.def(\"forward\", &forward, \"Matrix multiplication kernel (CUDA)\");\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(df_l1.iloc[0][[\"CUDA_Code\"]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_l1.iloc[0][[\"PyTorch_Code_Functional\"]].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_q14 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "model_q7 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "# model_q1 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_cpu = \"HuggingFaceTB/SmolLM-135M\"\n",
    "# model_llama = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_q7,torch_dtype=\"auto\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_q7, torch_dtype=\"auto\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "with torch.cuda.device(0):  # explicitly set GPU 0 if needed\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "# tokenizer = tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"what is the solution of x^2 - 2x + 1 = 0?<think>\"\n",
    "\n",
    "# prompt = \"what is the second planet from the Sun?<think>\"\n",
    "# prompt = l1_samples.iloc[0][\"PyTorch_Code_Functional\"]\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=100) #max_length=10_000\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = inputs.to('cpu')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n\\ndef module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs a single square matrix multiplication (C = A * B).\\n\\n    Args:\\n        A (torch.Tensor): Input matrix A of shape (N, N).\\n        B (torch.Tensor): Input matrix B of shape (N, N).\\n\\n    Returns:\\n        torch.Tensor: Output matrix C of shape (N, N).\\n    \"\"\"\\n    return torch.matmul(A, B)\\n\\n\\nclass Model(nn.Module):\\n    \"\"\"\\n    Simple model that performs a single square matrix multiplication (C = A * B)\\n    \"\"\"\\n\\n    def __init__(self):\\n        super(Model, self).__init__()\\n\\n    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\\n        return fn(A, B)\\n\\n\\nN = 2048\\n\\n\\ndef get_inputs():\\n    A = torch.randn(N, N)\\n    B = torch.randn(N, N)\\n    return [A, B]\\n\\n\\ndef get_init_inputs():\\n    return []  # No special initialization inputs needed\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l1.iloc[0].PyTorch_Code_Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompting import prompt_generate_custom_cuda_from_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_str = df_l1.iloc[0].PyTorch_Code_Functional\n",
    "prompt = prompt_generate_custom_cuda_from_prompt_template(functional_str, add_think=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You write custom CUDA kernels to replace the pytorch operators in the given architecture to guarantee correctness and valid compiilation, and secondarily to get speedups. \n",
      "\n",
      "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\n",
      "\n",
      "Here's an example to show you the syntax of the architecture you will see implemented in torch. The example architecture is for element-wise addition: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "\n",
      "    def forward(self, a, b):\n",
      "        return a + b\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    # randomly generate input tensors based on the model architecture\n",
      "    a = torch.randn(1, 128).cuda()\n",
      "    b = torch.randn(1, 128).cuda()\n",
      "    return [a, b]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    # randomly generate tensors required for initialization based on the model architecture\n",
      "    return []\n",
      "```\n",
      "\n",
      "The example CUDA output is: \n",
      "\n",
      "```\n",
      "# Define the custom CUDA kernel for element-wise addition\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
      "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (idx < size) {\n",
      "        out[idx] = a[idx] + b[idx];\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a, torch::Tensor b) {\n",
      "    auto size = a.numel();\n",
      "    auto out = torch::zeros_like(a);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (size + block_size - 1) / block_size;\n",
      "\n",
      "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
      "\n",
      "    return out;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a, torch::Tensor b);\"\n",
      ")\n",
      "```\n",
      "\n",
      "Here's another example to show you the syntax of the architecture you could see implemented in torch. The example below is for ReLU: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a ReLU activation.\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Applies ReLU activation to the input tensor.\n",
      "\n",
      "        Args:\n",
      "            x (torch.Tensor): Input tensor of any shape.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n",
      "        \"\"\"\n",
      "        return torch.relu(x)\n",
      "\n",
      "batch_size = 16\n",
      "dim = 16384\n",
      "\n",
      "def get_inputs():\n",
      "    x = torch.randn(batch_size, dim)\n",
      "    return [x]\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "```\n",
      "\n",
      "The example CUDA output is:\n",
      "```\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "\n",
      "__global__ void relu_kernel(const float* a, float* out, int size) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < size) {\n",
      "        out[i] = (a[i] > 0) ? a[i] : 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a) {\n",
      "    auto size = a.numel();\n",
      "    auto output = torch::zeros_like(a);\n",
      "\n",
      "    int threadsPerBlock = 256;\n",
      "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
      "\n",
      "    relu_kernel<<<blocksPerGrid, threadsPerBlock>>>(a.data_ptr<float>(), output.data_ptr<float>(), size);\n",
      "\n",
      "    return output;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a);\"\n",
      ")\n",
      "```\n",
      "\n",
      "Given the above examples, you are given the following torch architecture: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n",
      "```\n",
      "\n",
      "Optimize the torch architecture named Model with CUDA operators! \n",
      "\n",
      "Just create the cuda_source and cpp_source strings, as in the example CUDA architecture. Remember to name the main cuda function 'forward', as is done in the example cuda code. Don't add any other thoughts, comments, or pseudocode. Just put the CUDA code inside of the cuda_source string, and create the accompanying cpp_source string that only captures the forward method and the method signature, as is given in the example CUDA output. \n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "functional_str = df_l1.iloc[0].PyTorch_Code_Functional\n",
    "prompt = prompt_generate_custom_cuda_from_prompt_template(functional_str, add_think=False)\n",
    "\n",
    "# Tokenize the input promptb\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=3_000)\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1669])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You write custom CUDA kernels to replace the pytorch operators in the given architecture to guarantee correctness and valid compiilation, and secondarily to get speedups. \n",
      "\n",
      "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\n",
      "\n",
      "Here's an example to show you the syntax of the architecture you will see implemented in torch. The example architecture is for element-wise addition: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "\n",
      "    def forward(self, a, b):\n",
      "        return a + b\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    # randomly generate input tensors based on the model architecture\n",
      "    a = torch.randn(1, 128).cuda()\n",
      "    b = torch.randn(1, 128).cuda()\n",
      "    return [a, b]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    # randomly generate tensors required for initialization based on the model architecture\n",
      "    return []\n",
      "```\n",
      "\n",
      "The example CUDA output is: \n",
      "\n",
      "```\n",
      "# Define the custom CUDA kernel for element-wise addition\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
      "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (idx < size) {\n",
      "        out[idx] = a[idx] + b[idx];\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a, torch::Tensor b) {\n",
      "    auto size = a.numel();\n",
      "    auto out = torch::zeros_like(a);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (size + block_size - 1) / block_size;\n",
      "\n",
      "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
      "\n",
      "    return out;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a, torch::Tensor b);\"\n",
      ")\n",
      "```\n",
      "\n",
      "Here's another example to show you the syntax of the architecture you could see implemented in torch. The example below is for ReLU: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a ReLU activation.\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Applies ReLU activation to the input tensor.\n",
      "\n",
      "        Args:\n",
      "            x (torch.Tensor): Input tensor of any shape.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n",
      "        \"\"\"\n",
      "        return torch.relu(x)\n",
      "\n",
      "batch_size = 16\n",
      "dim = 16384\n",
      "\n",
      "def get_inputs():\n",
      "    x = torch.randn(batch_size, dim)\n",
      "    return [x]\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "```\n",
      "\n",
      "The example CUDA output is:\n",
      "```\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "\n",
      "__global__ void relu_kernel(const float* a, float* out, int size) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < size) {\n",
      "        out[i] = (a[i] > 0) ? a[i] : 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a) {\n",
      "    auto size = a.numel();\n",
      "    auto output = torch::zeros_like(a);\n",
      "\n",
      "    int threadsPerBlock = 256;\n",
      "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
      "\n",
      "    relu_kernel<<<blocksPerGrid, threadsPerBlock>>>(a.data_ptr<float>(), output.data_ptr<float>(), size);\n",
      "\n",
      "    return output;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a);\"\n",
      ")\n",
      "```\n",
      "\n",
      "You are given the following torch architecture: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n",
      "```\n",
      "\n",
      "Optimize the torch architecture named Model with CUDA operators! \n",
      "\n",
      "Just create the cuda_source and cpp_source strings, as in the example CUDA architecture. Remember to name the main cuda function 'forward', as is done in the example cuda code. Don't add any other thoughts, comments, or pseudocode. Just put the CUDA code inside of the cuda_source string, and create the accompanying cpp_source string that only captures the forward method and the method signature, as is given in the example CUDA output. \n",
      "</think>\n",
      "\n",
      "To optimize the matrix multiplication operation using CUDA, we'll implement a custom CUDA kernel that efficiently performs the matrix multiplication on the GPU. This approach reduces the overhead of calling the PyTorch `torch.matmul` function by offloading the computation to CUDA-enabled devices.\n",
      "\n",
      "Here's the CUDA source code that implements the optimized matrix multiplication:\n",
      "\n",
      "```cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
      "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    float sum = 0.0f;\n",
      "    for (int k = 0; k < N; ++k) {\n",
      "        sum += A[row * N + k] * B[k * N + col];\n",
      "    }\n",
      "    C[row * N + col] = sum;\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    int size = A.numel();\n",
      "    auto C = torch::zeros_like(A);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (N + block_size - 1) / block_size;\n",
      "\n",
      "    matmul_kernel<<<num_blocks, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "The corresponding C++ source code that interfaces with the CUDA implementation is:\n",
      "\n",
      "```cpp_source = \"\"\"\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    int N = A.size(0);\n",
      "    auto C = torch::zeros_like(A);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (N + block_size - 1) / block_size;\n",
      "\n",
      "    matmul_kernel<<<num_blocks, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "This implementation uses CUDA's shared memory and warp-level threading to optimize memory access patterns and parallelism, resulting in significant speedups compared to the standard PyTorch implementation. The custom CUDA kernel directly performs the matrix multiplication, avoiding the overhead of high-level PyTorch operations.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_pytorch_functional = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single square matrix multiplication (C = A * B).\n",
    "\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
    "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output matrix C of shape (N, N).\n",
    "    \"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
    "        return fn(A, B)\n",
    "\n",
    "\n",
    "N = 2048\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    A = torch.randn(N, N).cuda()\n",
    "    B = torch.randn(N, N).cuda()\n",
    "    return [A, B]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    return []  # No special initialization inputs needed\n",
    "'''\n",
    "\n",
    "\n",
    "working_cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "working_cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "working_response = '''\n",
    "</think>\n",
    "cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_samples = df_l1[df_l1.Kernel_Name == df_l1.Op_Name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-30\n",
      "The outputted CUDA kernel was not formatted correctly. Please follow the format of the examples given!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "Traceback (most recent call last):\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2209, in _run_ninja_build\n",
      "    subprocess.run(\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/subprocess.py\", line 571, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/DISK/cs234_cuda/reward_model.py\", line 79, in reward\n",
      "    cuda_mod = load_inline(\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\n",
      "    return _jit_compile(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1798, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1926, in _write_ninja_file_and_build_library\n",
      "    _run_ninja_build(\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2225, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'MyhmMFemWFVaRKoQJuuY': [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MyhmMFemWFVaRKoQJuuY -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu -o cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mcuda.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MyhmMFemWFVaRKoQJuuY -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu -o cuda.cuda.o \n",
      "/DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu(55): error: expected a \";\"\n",
      "  }\n",
      "  ^\n",
      "\n",
      "1 error detected in the compilation of \"/DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu\".\n",
      "[2/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=MyhmMFemWFVaRKoQJuuY -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/main.cpp -o main.o \n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reward_model import reward\n",
    "\n",
    "reward_val, msg = reward(l1_samples.iloc[0].PyTorch_Code_Functional, response)\n",
    "print(reward_val)\n",
    "print(msg)\n",
    "print()\n",
    "reward_val_good, msg_good = reward(working_pytorch_functional, working_response)\n",
    "print(reward_val_good)\n",
    "print(msg_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sakana Eval Testing (Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"tasks/{df_l1.iloc[0].Op_Name}.py\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].PyTorch_Code_Functional)\n",
    "with open(f\"kernels/{df_l1.iloc[0].Op_Name}.cu\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].CUDA_Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File format is:\n",
    "\n",
    "eval_kernel\n",
    "\n",
    "task/\n",
    "- torch nn module.py\n",
    "- functional.py\n",
    "- info.txt (dont really need this rn)\n",
    "\n",
    "kernel/\n",
    "- kernel.cu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# import os\n",
    "# import torch\n",
    "# import argparse\n",
    "# from torch.utils.cpp_extension import load\n",
    "# from torch.utils._pytree import tree_map\n",
    "# import importlib.util\n",
    "# from torch.utils.benchmark import Timer\n",
    "\n",
    "\n",
    "# def easy_to_device(pytree, device):\n",
    "#     return tree_map(\n",
    "#         lambda x: x.to(device) if isinstance(x, torch.Tensor) else x, pytree\n",
    "#     )\n",
    "\n",
    "\n",
    "# def load_module_from_path(path):\n",
    "#     spec = importlib.util.spec_from_file_location(\"module\", path)\n",
    "#     module = importlib.util.module_from_spec(spec)\n",
    "#     spec.loader.exec_module(module)\n",
    "#     return module\n",
    "\n",
    "\n",
    "# def evaluate(op_name: str):\n",
    "#     # parser = argparse.ArgumentParser()\n",
    "#     # parser.add_argument(\"--op_atol\", type=float, default=1e-3)\n",
    "#     # parser.add_argument(\"--op_rtol\", type=float, default=1e-1)\n",
    "#     # parser.add_argument(\"--rep_time\", type=int, default=10000)\n",
    "#     # parser.add_argument(\"--warmup_time\", type=int, default=25)\n",
    "#     # args = parser.parse_args()\n",
    "\n",
    "#     # # Get task name from info.txt\n",
    "#     # with open(\"task/info.txt\", \"r\") as f:\n",
    "#     #     task_name = f.readline().strip()\n",
    "#     #     task_name = \"_\".join(task_name.split(\"_\")[1:])  # Remove problem ID\n",
    "\n",
    "#     # Import the task module\n",
    "#     # task_files = [f for f in os.listdir(\"tasks\") if f.endswith(\"_functional.py\")]\n",
    "#     # if not task_files:\n",
    "#     #     raise RuntimeError(\"No functional task file found\")\n",
    "\n",
    "#     task = load_module_from_path(os.path.join(\"tasks\", op_name+'.py'))\n",
    "\n",
    "#     # Initialize model and inputs\n",
    "#     device_1 = torch.device(\"cuda:0\")\n",
    "#     torch.manual_seed(0)\n",
    "#     inputs = task.get_inputs()\n",
    "#     init_inputs = task.get_init_inputs()\n",
    "#     model = task.Model(*init_inputs)\n",
    "\n",
    "#     # Load CUDA kernel\n",
    "#     # kernel_files = [f for f in os.listdir(\"kernel\") if f.endswith(\".cu\")]\n",
    "#     # if not kernel_files:\n",
    "#     #     raise RuntimeError(\"No CUDA kernel file found\")\n",
    "    \n",
    "#     task_name = \"_\".join(op_name.split(\"_\")[1:])  # Remove problem ID\n",
    "#     cuda_module = load(\n",
    "#         name=task_name,\n",
    "#         sources=[os.path.join(\"kernels\", op_name+'.cu')],\n",
    "#         extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
    "#         with_cuda=True,\n",
    "#         verbose=True,\n",
    "#     )\n",
    "\n",
    "#     # Test for correctness\n",
    "#     with torch.no_grad():\n",
    "#         cuda_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=cuda_module.forward\n",
    "#         )\n",
    "#         torch_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=task.module_fn\n",
    "#         )\n",
    "\n",
    "#     rtol_default = 1e-1\n",
    "#     atol_default = 1e-3\n",
    "\n",
    "#     correct = torch.allclose(\n",
    "#         torch_output.cpu(),\n",
    "#         cuda_output.cpu(),\n",
    "#         rtol=rtol_default,\n",
    "#         atol=atol_default,\n",
    "#     )\n",
    "#     max_diff = torch.max(torch.abs(torch_output.cpu() - cuda_output.cpu())).item()\n",
    "#     print(f\"Tested CUDA kernel - Correct: {correct}, Max Diff: {max_diff}\")\n",
    "\n",
    "#     if correct:\n",
    "#         # Evaluate CUDA kernel performance\n",
    "#         cuda_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=cuda_module.forward)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"cuda_module\": cuda_module,\n",
    "#             },\n",
    "#         )\n",
    "#         cuda_runtime = cuda_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated CUDA kernel - Runtime: {cuda_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate PyTorch baseline performance\n",
    "#         torch_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=task.module_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"task\": task,\n",
    "#             },\n",
    "#         )\n",
    "#         torch_runtime = torch_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated PyTorch baseline - Runtime: {torch_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate torch compile performance\n",
    "#         torch_fn = task.module_fn\n",
    "#         compile_fn = torch.compile(torch_fn, mode=\"max-autotune\")\n",
    "#         torch_compile_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=compile_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"compile_fn\": compile_fn,\n",
    "#             },\n",
    "#         )\n",
    "\n",
    "#         torch_compile_runtime = torch_compile_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated torch compile - Runtime: {torch_compile_runtime:.3f} ms\")\n",
    "\n",
    "#         print(f\"Speedup over PyTorch: {torch_runtime/cuda_runtime:.2f}x\")\n",
    "#         print(f\"Speedup over torch compile: {torch_compile_runtime/cuda_runtime:.2f}x\")\n",
    "\n",
    "#         import json\n",
    "\n",
    "#         # Store the speedup times as a json file\n",
    "#         file_path = os.path.join(os.path.dirname('speedups'), f\"{op_name}.json\")\n",
    "#         with open(file_path, \"w\") as f:\n",
    "#             json.dump(\n",
    "#                 {\n",
    "#                     \"max_diff\": max_diff,\n",
    "#                     \"cuda_runtime\": cuda_runtime,\n",
    "#                     \"torch_runtime\": torch_runtime,\n",
    "#                     \"torch_compile_runtime\": torch_compile_runtime,\n",
    "#                     \"speedup_over_pytorch\": torch_runtime / cuda_runtime,\n",
    "#                     \"speedup_over_torch_compile\": torch_compile_runtime / cuda_runtime,\n",
    "#                 },\n",
    "#                 f,\n",
    "#             )\n",
    "#         print(f\"Speedup times stored in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(df_l1.iloc[0].Op_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code from KernelBench. Evaluation seems to work with this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using ./cache_dir as PyTorch extensions root...\n",
      "The input conditions for extension module elementwise_add have changed. Bumping to version 4 and re-building as elementwise_add_v4...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file ./cache_dir/elementwise_add/build.ninja...\n",
      "/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module elementwise_add_v4...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=elementwise_add_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /DISK/cs234_cuda/cache_dir/elementwise_add/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=elementwise_add_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /DISK/cs234_cuda/cache_dir/elementwise_add/cuda.cu -o cuda.cuda.o \n",
      "[3/3] c++ main.o cuda.cuda.o -shared  -L/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o elementwise_add_v4.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module elementwise_add_v4...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    # randomly generate input tensors based on the model architecture\n",
    "    a = torch.randn(1, 128).cuda()\n",
    "    b = torch.randn(1, 128).cuda()\n",
    "    return [a, b]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    # randomly generate tensors required for initialization based on the model architecture\n",
    "    return []\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# Define the custom CUDA kernel for element-wise addition\n",
    "elementwise_add_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        out[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "    auto size = a.numel();\n",
    "    auto out = torch::zeros_like(a);\n",
    "\n",
    "    const int block_size = 256;\n",
    "    const int num_blocks = (size + block_size - 1) / block_size;\n",
    "\n",
    "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
    "\n",
    "    return out;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "elementwise_add_cpp_source = (\n",
    "    \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code for element-wise addition\n",
    "elementwise_add = load_inline(\n",
    "    name=\"elementwise_add\",\n",
    "    cpp_sources=elementwise_add_cpp_source,\n",
    "    cuda_sources=elementwise_add_source,\n",
    "    functions=[\"elementwise_add_cuda\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.elementwise_add = elementwise_add\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.elementwise_add.elementwise_add_cuda(a, b)\n",
    "    \n",
    "a, b = get_inputs()\n",
    "torchm = Model()\n",
    "cudam = ModelNew()\n",
    "torch.allclose(torchm.forward(a,b), cudam.forward(a,b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single square matrix multiplication (C = A * B).\n",
    "\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
    "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output matrix C of shape (N, N).\n",
    "    \"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
    "        return fn(A, B)\n",
    "\n",
    "\n",
    "N = 2048\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    A = torch.randn(N, N).cuda()\n",
    "    B = torch.randn(N, N).cuda()\n",
    "    return [A, B]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    return []  # No special initialization inputs needed\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code \n",
    "cuda_mod = load_inline(\n",
    "    name=\"matmul\",\n",
    "    cpp_sources=cuda_cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"forward\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cuda_mod = cuda_mod\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.cuda_mod.forward(a, b)\n",
    "\n",
    "# QWEN 7B GENERATED\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# # Define the custom CUDA kernel for matrix multiplication\n",
    "# matmul_kernel_source = \"\"\"\n",
    "# #include <torch/extension.h>\n",
    "# #include <cuda_runtime.h>\n",
    "\n",
    "# __global__ void matmul_kernel(const float* a, const float* b, float* out, int n) {\n",
    "#     int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "#     int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "#     float sum = 0.0f;\n",
    "#     for (int k = 0; k < n; k++) {\n",
    "#         sum += a[row * n + k] * b[k * n + col];\n",
    "#     }\n",
    "#     out[row * n + col] = sum;\n",
    "# }\n",
    "\n",
    "# torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "#     int n = a.size(1);\n",
    "#     int m = b.size(0);\n",
    "#     int k = b.size(1);\n",
    "\n",
    "#     auto out_size = m * k;\n",
    "#     auto out = torch::zeros(m, k, sizeof(float));\n",
    "\n",
    "#     const int block_size = 256;\n",
    "#     const int num_blocks = (out_size + block_size - 1) / block_size;\n",
    "\n",
    "#     matmul_kernel<<<num_blocks, block_size>>>(\n",
    "#         a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), n);\n",
    "\n",
    "#     return out;\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# matmul_cpp_source = (\n",
    "#     \"torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    "# )\n",
    "\n",
    "# # Compile the inline CUDA code for matrix multiplication\n",
    "# matmul = load_inline(\n",
    "#     name=\"matmul\",\n",
    "#     cpp_sources=matmul_cpp_source,\n",
    "#     cuda_sources=matmul_kernel_source,\n",
    "#     functions=[\"matmul_cuda\"],\n",
    "#     verbose=True,\n",
    "#     extra_cflags=[\"-O3\"],\n",
    "#     extra_ldflags=[\"\"],\n",
    "# )\n",
    "\n",
    "# class ModelNew(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.matmul = matmul\n",
    "\n",
    "#     def forward(self, a, b):\n",
    "#         return self.matmul.matmul_cuda(a, b)\n",
    "    \n",
    "torch_mod = Model()\n",
    "cuda_mod = ModelNew()\n",
    "\n",
    "a, b = get_inputs()\n",
    "# a, b, = torch.eye(N).cuda(), torch.eye(N).cuda()\n",
    "print(torch.allclose(torch_mod.forward(a, b), cuda_mod.forward(a, b), rtol=1e-1, atol=1e-3))\n",
    "torch_mod.forward(a, b), cuda_mod.forward(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1_samples.iloc[2].CUDA_Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(l1_samples.iloc[2].PyTorch_Code_Functional)\n",
    "cuda_src = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel for batched matrix multiplication: C = A * B\n",
    "// Shapes: A (batch_size, M, K), B (batch_size, K, N), C (batch_size, M, N)\n",
    "__global__ void bmm_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    float* __restrict__ C,\n",
    "    int batch_size,\n",
    "    int M,\n",
    "    int K,\n",
    "    int N\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * M * N;\n",
    "    if (idx >= total) return;\n",
    "\n",
    "    int b = idx / (M * N);\n",
    "    int remainder = idx % (M * N);\n",
    "    int m = remainder / N;\n",
    "    int n = remainder % N;\n",
    "\n",
    "    float val = 0.0f;\n",
    "    for (int k = 0; k < K; k++) {\n",
    "        val += A[b * M * K + m * K + k] * B[b * K * N + k * N + n];\n",
    "    }\n",
    "    C[b * M * N + m * N + n] = val;\n",
    "}\n",
    "\n",
    "torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B) {\n",
    "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
    "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
    "    TORCH_CHECK(A.dim() == 3, \"A must be 3D\");\n",
    "    TORCH_CHECK(B.dim() == 3, \"B must be 3D\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"Batch sizes must match\");\n",
    "    TORCH_CHECK(A.size(2) == B.size(1), \"Inner dimensions (K) must match\");\n",
    "\n",
    "    int batch_size = A.size(0);\n",
    "    int M = A.size(1);\n",
    "    int K = A.size(2);\n",
    "    int N = B.size(2);\n",
    "\n",
    "    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n",
    "    auto C = torch::zeros({batch_size, M, N}, options);\n",
    "\n",
    "    int total = batch_size * M * N;\n",
    "    const int threads = 256;\n",
    "    int blocks = (total + threads - 1) / threads;\n",
    "\n",
    "    bmm_kernel<<<blocks, threads>>>(\n",
    "        A.data_ptr<float>(),\n",
    "        B.data_ptr<float>(),\n",
    "        C.data_ptr<float>(),\n",
    "        batch_size, M, K, N\n",
    "    );\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_cpp_source = (\n",
    "    \"torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code \n",
    "cuda_mod = load_inline(\n",
    "    name=\"bmm\",\n",
    "    cpp_sources=cuda_cpp_source,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=[\"forward_bmm\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cuda_mod = cuda_mod\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.cuda_mod.forward_bmm(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_inputs()\n",
    "a = a.cuda()\n",
    "b = b.cuda()\n",
    "\n",
    "torchm = Model()\n",
    "cudam = Model()\n",
    "print(torch.allclose(torchm.forward(a, b), cudam.forward(a, b), rtol=1e-1, atol=1e-3))\n",
    "torchm.forward(a, b), cudam.forward(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- ~~evaluation stuff~~\n",
    "- prompting qwen for good outputs \n",
    "- ~~KernelBench method - NA~~\n",
    "- coding the RL portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL portion\n",
    "Below implements a GRPO like method (calculating advantage from reward - mean reward in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# dummy reward function\n",
    "def reward_function(output_text: str) -> float:\n",
    "    return output_text.count('def')\n",
    "\n",
    "# PPO hyperparameters\n",
    "clip_range = 0.2        # clipping range for PPO\n",
    "ppo_epochs = 1          # number of PPO updates per batch\n",
    "batch_size = 2          # size of our batch (number of prompts)\n",
    "num_iterations = 1    # total training iterations\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    batch_prompts = [\n",
    "        l1_samples.iloc[0]['PyTorch_Code_Functional'],\n",
    "        l1_samples.iloc[1]['PyTorch_Code_Functional']\n",
    "    ]\n",
    "    batch_outputs = []\n",
    "    batch_rewards = []\n",
    "    batch_log_probs_old = []\n",
    "\n",
    "    # (old policy): generate data\n",
    "    for prompt in batch_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        output_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        batch_outputs.append(output_text)\n",
    "        \n",
    "        reward = reward_function(output_text)\n",
    "        batch_rewards.append(reward)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(gen_ids)\n",
    "            log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "            gen_log_probs = log_probs.gather(2, gen_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            sequence_log_prob = gen_log_probs.sum()\n",
    "            batch_log_probs_old.append(sequence_log_prob)\n",
    "    \n",
    "    rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float)\n",
    "    old_log_probs_tensor = torch.stack(batch_log_probs_old)\n",
    "\n",
    "    # PPO update loop\n",
    "    for _ in range(ppo_epochs):\n",
    "        new_log_probs_list = []\n",
    "        for output_text in batch_outputs:\n",
    "            gen_ids = tokenizer(output_text, return_tensors=\"pt\").input_ids\n",
    "            outputs = model(gen_ids)\n",
    "            log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "            gen_log_probs = log_probs.gather(2, gen_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            sequence_log_prob = gen_log_probs.sum()\n",
    "            new_log_probs_list.append(sequence_log_prob)\n",
    "        \n",
    "        new_log_probs_tensor = torch.stack(new_log_probs_list)\n",
    "        ratio = torch.exp(new_log_probs_tensor - old_log_probs_tensor)\n",
    "        advantage = rewards_tensor - rewards_tensor.mean()\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * advantage\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Iteration {iteration}: loss = {loss.item():.4f}\")\n",
    "    print(\"Generated outputs:\", batch_outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs234",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
