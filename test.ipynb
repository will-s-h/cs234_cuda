{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DISK/conda_envs/cs234/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    level_1: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12157\n",
      "    })\n",
      "    level_2: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 12938\n",
      "    })\n",
      "    level_3: Dataset({\n",
      "        features: ['Op_Name', 'Level_ID', 'Task_ID', 'Kernel_Name', 'CUDA_Runtime', 'PyTorch_Native_Runtime', 'PyTorch_Compile_Runtime', 'CUDA_Speedup_Native', 'CUDA_Speedup_Compile', 'CUDA_Code', 'PyTorch_Code_Module', 'PyTorch_Code_Functional', 'Correct', 'Max_Diff', 'Error', 'NCU_Profile', 'Torch_Profile', 'Clang_Tidy', '__index_level_0__'],\n",
      "        num_rows: 5520\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "# Specify the dataset name and the cache directory\n",
    "dataset_name = \"SakanaAI/AI-CUDA-Engineer-Archive\"\n",
    "cache_dir = \"./cache_dir\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ['TORCH_EXTENSIONS_DIR'] = cache_dir\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "\n",
    "# Print the dataset to verify\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = dataset[\"level_1\"].to_pandas()\n",
    "l1_samples = df_l1[df_l1.Kernel_Name == df_l1.Op_Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_q14 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "# model_q7 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "model_q1 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_cpu = \"HuggingFaceTB/SmolLM-135M\"\n",
    "# model_llama = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_q1, torch_dtype=\"auto\", load_in_8bit=True, device_map=\"auto\", cache_dir=cache_dir)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_q1, torch_dtype=\"auto\", load_in_8bit=True, device_map=\"auto\", cache_dir=cache_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_q1,torch_dtype=\"auto\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_q1, torch_dtype=\"auto\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "with torch.cuda.device(0):  # explicitly set GPU 0 if needed\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "# tokenizer = tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#max_length=10_000\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Decode the generated tokens to get the response\u001b[39;00m\n\u001b[32m     14\u001b[39m response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/generation/utils.py:2223\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2215\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2216\u001b[39m         input_ids=input_ids,\n\u001b[32m   2217\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2218\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2219\u001b[39m         **model_kwargs,\n\u001b[32m   2220\u001b[39m     )\n\u001b[32m   2222\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2223\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2224\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2228\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2233\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2234\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2235\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2236\u001b[39m         batch_size=batch_size,\n\u001b[32m   2237\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2242\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/generation/utils.py:3214\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3212\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3214\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3216\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3217\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3218\u001b[39m     outputs,\n\u001b[32m   3219\u001b[39m     model_kwargs,\n\u001b[32m   3220\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3221\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    853\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    871\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    568\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    569\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m         position_embeddings,\n\u001b[32m    577\u001b[39m     )\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:257\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    244\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    245\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m     **kwargs: Unpack[FlashAttentionKwargs],\n\u001b[32m    254\u001b[39m ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\u001b[32m    255\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    260\u001b[39m     hidden_states, self_attn_weights = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    261\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    262\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m         **kwargs,\n\u001b[32m    270\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DISK/conda_envs/cs234/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:220\u001b[39m, in \u001b[36mQwen2RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    219\u001b[39m     input_dtype = hidden_states.dtype\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     hidden_states = \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    222\u001b[39m     hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"what is the solution of x^2 - 2x + 1 = 0?<think>\"\n",
    "\n",
    "# prompt = \"what is the second planet from the Sun?<think>\"\n",
    "# prompt = l1_samples.iloc[0][\"PyTorch_Code_Functional\"]\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=100) #max_length=10_000\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = inputs.to('cpu')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n\\ndef module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs a single square matrix multiplication (C = A * B).\\n\\n    Args:\\n        A (torch.Tensor): Input matrix A of shape (N, N).\\n        B (torch.Tensor): Input matrix B of shape (N, N).\\n\\n    Returns:\\n        torch.Tensor: Output matrix C of shape (N, N).\\n    \"\"\"\\n    return torch.matmul(A, B)\\n\\n\\nclass Model(nn.Module):\\n    \"\"\"\\n    Simple model that performs a single square matrix multiplication (C = A * B)\\n    \"\"\"\\n\\n    def __init__(self):\\n        super(Model, self).__init__()\\n\\n    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\\n        return fn(A, B)\\n\\n\\nN = 2048\\n\\n\\ndef get_inputs():\\n    A = torch.randn(N, N)\\n    B = torch.randn(N, N)\\n    return [A, B]\\n\\n\\ndef get_init_inputs():\\n    return []  # No special initialization inputs needed\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l1.iloc[0].PyTorch_Code_Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompting import prompt_generate_custom_cuda_from_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_str = df_l1.iloc[0].PyTorch_Code_Functional\n",
    "prompt = prompt_generate_custom_cuda_from_prompt_template(functional_str, add_think=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You write custom CUDA kernels to replace the pytorch operators in the given architecture to guarantee correctness and valid compiilation, and secondarily to get speedups. \n",
      "\n",
      "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\n",
      "\n",
      "Here's an example to show you the syntax of the architecture you will see implemented in torch. The example architecture is for element-wise addition: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "\n",
      "    def forward(self, a, b):\n",
      "        return a + b\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    # randomly generate input tensors based on the model architecture\n",
      "    a = torch.randn(1, 128).cuda()\n",
      "    b = torch.randn(1, 128).cuda()\n",
      "    return [a, b]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    # randomly generate tensors required for initialization based on the model architecture\n",
      "    return []\n",
      "```\n",
      "\n",
      "The example CUDA output is: \n",
      "\n",
      "```\n",
      "# Define the custom CUDA kernel for element-wise addition\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
      "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (idx < size) {\n",
      "        out[idx] = a[idx] + b[idx];\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a, torch::Tensor b) {\n",
      "    auto size = a.numel();\n",
      "    auto out = torch::zeros_like(a);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (size + block_size - 1) / block_size;\n",
      "\n",
      "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
      "\n",
      "    return out;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a, torch::Tensor b);\"\n",
      ")\n",
      "```\n",
      "\n",
      "Here's another example to show you the syntax of the architecture you could see implemented in torch. The example below is for ReLU: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a ReLU activation.\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Applies ReLU activation to the input tensor.\n",
      "\n",
      "        Args:\n",
      "            x (torch.Tensor): Input tensor of any shape.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n",
      "        \"\"\"\n",
      "        return torch.relu(x)\n",
      "\n",
      "batch_size = 16\n",
      "dim = 16384\n",
      "\n",
      "def get_inputs():\n",
      "    x = torch.randn(batch_size, dim)\n",
      "    return [x]\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "```\n",
      "\n",
      "The example CUDA output is:\n",
      "```\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "\n",
      "__global__ void relu_kernel(const float* a, float* out, int size) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < size) {\n",
      "        out[i] = (a[i] > 0) ? a[i] : 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a) {\n",
      "    auto size = a.numel();\n",
      "    auto output = torch::zeros_like(a);\n",
      "\n",
      "    int threadsPerBlock = 256;\n",
      "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
      "\n",
      "    relu_kernel<<<blocksPerGrid, threadsPerBlock>>>(a.data_ptr<float>(), output.data_ptr<float>(), size);\n",
      "\n",
      "    return output;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a);\"\n",
      ")\n",
      "```\n",
      "\n",
      "Given the above examples, you are given the following torch architecture: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n",
      "```\n",
      "\n",
      "Optimize the torch architecture named Model with CUDA operators! \n",
      "\n",
      "Just create the cuda_source and cpp_source strings, as in the example CUDA architecture. Remember to name the main cuda function 'forward', as is done in the example cuda code. Don't add any other thoughts, comments, or pseudocode. Just put the CUDA code inside of the cuda_source string, and create the accompanying cpp_source string that only captures the forward method and the method signature, as is given in the example CUDA output. \n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "functional_str = df_l1.iloc[0].PyTorch_Code_Functional\n",
    "prompt = prompt_generate_custom_cuda_from_prompt_template(functional_str, add_think=False)\n",
    "\n",
    "# Tokenize the input promptb\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=3_000)\n",
    "\n",
    "# Decode the generated tokens to get the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1669])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You write custom CUDA kernels to replace the pytorch operators in the given architecture to guarantee correctness and valid compiilation, and secondarily to get speedups. \n",
      "\n",
      "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\n",
      "\n",
      "Here's an example to show you the syntax of the architecture you will see implemented in torch. The example architecture is for element-wise addition: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "\n",
      "    def forward(self, a, b):\n",
      "        return a + b\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    # randomly generate input tensors based on the model architecture\n",
      "    a = torch.randn(1, 128).cuda()\n",
      "    b = torch.randn(1, 128).cuda()\n",
      "    return [a, b]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    # randomly generate tensors required for initialization based on the model architecture\n",
      "    return []\n",
      "```\n",
      "\n",
      "The example CUDA output is: \n",
      "\n",
      "```\n",
      "# Define the custom CUDA kernel for element-wise addition\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
      "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (idx < size) {\n",
      "        out[idx] = a[idx] + b[idx];\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a, torch::Tensor b) {\n",
      "    auto size = a.numel();\n",
      "    auto out = torch::zeros_like(a);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (size + block_size - 1) / block_size;\n",
      "\n",
      "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
      "\n",
      "    return out;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a, torch::Tensor b);\"\n",
      ")\n",
      "```\n",
      "\n",
      "Here's another example to show you the syntax of the architecture you could see implemented in torch. The example below is for ReLU: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a ReLU activation.\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Applies ReLU activation to the input tensor.\n",
      "\n",
      "        Args:\n",
      "            x (torch.Tensor): Input tensor of any shape.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n",
      "        \"\"\"\n",
      "        return torch.relu(x)\n",
      "\n",
      "batch_size = 16\n",
      "dim = 16384\n",
      "\n",
      "def get_inputs():\n",
      "    x = torch.randn(batch_size, dim)\n",
      "    return [x]\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "```\n",
      "\n",
      "The example CUDA output is:\n",
      "```\n",
      "cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "\n",
      "__global__ void relu_kernel(const float* a, float* out, int size) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < size) {\n",
      "        out[i] = (a[i] > 0) ? a[i] : 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor a) {\n",
      "    auto size = a.numel();\n",
      "    auto output = torch::zeros_like(a);\n",
      "\n",
      "    int threadsPerBlock = 256;\n",
      "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
      "\n",
      "    relu_kernel<<<blocksPerGrid, threadsPerBlock>>>(a.data_ptr<float>(), output.data_ptr<float>(), size);\n",
      "\n",
      "    return output;\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "cpp_source = (\n",
      "    \"torch::Tensor forward(torch::Tensor a);\"\n",
      ")\n",
      "```\n",
      "\n",
      "You are given the following torch architecture: \n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a single square matrix multiplication (C = A * B).\n",
      "\n",
      "    Args:\n",
      "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
      "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output matrix C of shape (N, N).\n",
      "    \"\"\"\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    \"\"\"\n",
      "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "\n",
      "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
      "        return fn(A, B)\n",
      "\n",
      "\n",
      "N = 2048\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    A = torch.randn(N, N)\n",
      "    B = torch.randn(N, N)\n",
      "    return [A, B]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return []  # No special initialization inputs needed\n",
      "\n",
      "```\n",
      "\n",
      "Optimize the torch architecture named Model with CUDA operators! \n",
      "\n",
      "Just create the cuda_source and cpp_source strings, as in the example CUDA architecture. Remember to name the main cuda function 'forward', as is done in the example cuda code. Don't add any other thoughts, comments, or pseudocode. Just put the CUDA code inside of the cuda_source string, and create the accompanying cpp_source string that only captures the forward method and the method signature, as is given in the example CUDA output. \n",
      "</think>\n",
      "\n",
      "To optimize the matrix multiplication operation using CUDA, we'll implement a custom CUDA kernel that efficiently performs the matrix multiplication on the GPU. This approach reduces the overhead of calling the PyTorch `torch.matmul` function by offloading the computation to CUDA-enabled devices.\n",
      "\n",
      "Here's the CUDA source code that implements the optimized matrix multiplication:\n",
      "\n",
      "```cuda_source = \"\"\"\n",
      "#include <torch/extension.h>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
      "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    float sum = 0.0f;\n",
      "    for (int k = 0; k < N; ++k) {\n",
      "        sum += A[row * N + k] * B[k * N + col];\n",
      "    }\n",
      "    C[row * N + col] = sum;\n",
      "}\n",
      "\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    int size = A.numel();\n",
      "    auto C = torch::zeros_like(A);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (N + block_size - 1) / block_size;\n",
      "\n",
      "    matmul_kernel<<<num_blocks, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "The corresponding C++ source code that interfaces with the CUDA implementation is:\n",
      "\n",
      "```cpp_source = \"\"\"\n",
      "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
      "    int N = A.size(0);\n",
      "    auto C = torch::zeros_like(A);\n",
      "\n",
      "    const int block_size = 256;\n",
      "    const int num_blocks = (N + block_size - 1) / block_size;\n",
      "\n",
      "    matmul_kernel<<<num_blocks, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
      "\n",
      "    return C;\n",
      "}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "This implementation uses CUDA's shared memory and warp-level threading to optimize memory access patterns and parallelism, resulting in significant speedups compared to the standard PyTorch implementation. The custom CUDA kernel directly performs the matrix multiplication, avoiding the overhead of high-level PyTorch operations.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_pytorch_functional = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single square matrix multiplication (C = A * B).\n",
    "\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
    "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output matrix C of shape (N, N).\n",
    "    \"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
    "        return fn(A, B)\n",
    "\n",
    "\n",
    "N = 2048\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    A = torch.randn(N, N).cuda()\n",
    "    B = torch.randn(N, N).cuda()\n",
    "    return [A, B]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    return []  # No special initialization inputs needed\n",
    "'''\n",
    "\n",
    "\n",
    "working_cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "working_cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "working_response = '''\n",
    "</think>\n",
    "cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_samples = df_l1[df_l1.Kernel_Name == df_l1.Op_Name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-30\n",
      "The outputted CUDA kernel was not formatted correctly. Please follow the format of the examples given!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "Traceback (most recent call last):\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2209, in _run_ninja_build\n",
      "    subprocess.run(\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/subprocess.py\", line 571, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/DISK/cs234_cuda/reward_model.py\", line 79, in reward\n",
      "    cuda_mod = load_inline(\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\n",
      "    return _jit_compile(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1798, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1926, in _write_ninja_file_and_build_library\n",
      "    _run_ninja_build(\n",
      "  File \"/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2225, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'MyhmMFemWFVaRKoQJuuY': [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MyhmMFemWFVaRKoQJuuY -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu -o cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mcuda.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MyhmMFemWFVaRKoQJuuY -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu -o cuda.cuda.o \n",
      "/DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu(55): error: expected a \";\"\n",
      "  }\n",
      "  ^\n",
      "\n",
      "1 error detected in the compilation of \"/DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/cuda.cu\".\n",
      "[2/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=MyhmMFemWFVaRKoQJuuY -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /DISK/cs234_cuda/cache_dir/MyhmMFemWFVaRKoQJuuY/main.cpp -o main.o \n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reward_model import reward\n",
    "\n",
    "reward_val, msg = reward(l1_samples.iloc[0].PyTorch_Code_Functional, response)\n",
    "print(reward_val)\n",
    "print(msg)\n",
    "print()\n",
    "reward_val_good, msg_good = reward(working_pytorch_functional, working_response)\n",
    "print(reward_val_good)\n",
    "print(msg_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sakana Eval Testing (Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"tasks/{df_l1.iloc[0].Op_Name}.py\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].PyTorch_Code_Functional)\n",
    "with open(f\"kernels/{df_l1.iloc[0].Op_Name}.cu\", \"w\") as f:\n",
    "    f.write(df_l1.iloc[0].CUDA_Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File format is:\n",
    "\n",
    "eval_kernel\n",
    "\n",
    "task/\n",
    "- torch nn module.py\n",
    "- functional.py\n",
    "- info.txt (dont really need this rn)\n",
    "\n",
    "kernel/\n",
    "- kernel.cu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# # Evaluation script for CUDA kernel\n",
    "# # 12_Matmul_with_diagonal_matrices_\n",
    "# import os\n",
    "# import torch\n",
    "# import argparse\n",
    "# from torch.utils.cpp_extension import load\n",
    "# from torch.utils._pytree import tree_map\n",
    "# import importlib.util\n",
    "# from torch.utils.benchmark import Timer\n",
    "\n",
    "\n",
    "# def easy_to_device(pytree, device):\n",
    "#     return tree_map(\n",
    "#         lambda x: x.to(device) if isinstance(x, torch.Tensor) else x, pytree\n",
    "#     )\n",
    "\n",
    "\n",
    "# def load_module_from_path(path):\n",
    "#     spec = importlib.util.spec_from_file_location(\"module\", path)\n",
    "#     module = importlib.util.module_from_spec(spec)\n",
    "#     spec.loader.exec_module(module)\n",
    "#     return module\n",
    "\n",
    "\n",
    "# def evaluate(op_name: str):\n",
    "#     # parser = argparse.ArgumentParser()\n",
    "#     # parser.add_argument(\"--op_atol\", type=float, default=1e-3)\n",
    "#     # parser.add_argument(\"--op_rtol\", type=float, default=1e-1)\n",
    "#     # parser.add_argument(\"--rep_time\", type=int, default=10000)\n",
    "#     # parser.add_argument(\"--warmup_time\", type=int, default=25)\n",
    "#     # args = parser.parse_args()\n",
    "\n",
    "#     # # Get task name from info.txt\n",
    "#     # with open(\"task/info.txt\", \"r\") as f:\n",
    "#     #     task_name = f.readline().strip()\n",
    "#     #     task_name = \"_\".join(task_name.split(\"_\")[1:])  # Remove problem ID\n",
    "\n",
    "#     # Import the task module\n",
    "#     # task_files = [f for f in os.listdir(\"tasks\") if f.endswith(\"_functional.py\")]\n",
    "#     # if not task_files:\n",
    "#     #     raise RuntimeError(\"No functional task file found\")\n",
    "\n",
    "#     task = load_module_from_path(os.path.join(\"tasks\", op_name+'.py'))\n",
    "\n",
    "#     # Initialize model and inputs\n",
    "#     device_1 = torch.device(\"cuda:0\")\n",
    "#     torch.manual_seed(0)\n",
    "#     inputs = task.get_inputs()\n",
    "#     init_inputs = task.get_init_inputs()\n",
    "#     model = task.Model(*init_inputs)\n",
    "\n",
    "#     # Load CUDA kernel\n",
    "#     # kernel_files = [f for f in os.listdir(\"kernel\") if f.endswith(\".cu\")]\n",
    "#     # if not kernel_files:\n",
    "#     #     raise RuntimeError(\"No CUDA kernel file found\")\n",
    "    \n",
    "#     task_name = \"_\".join(op_name.split(\"_\")[1:])  # Remove problem ID\n",
    "#     cuda_module = load(\n",
    "#         name=task_name,\n",
    "#         sources=[os.path.join(\"kernels\", op_name+'.cu')],\n",
    "#         extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
    "#         with_cuda=True,\n",
    "#         verbose=True,\n",
    "#     )\n",
    "\n",
    "#     # Test for correctness\n",
    "#     with torch.no_grad():\n",
    "#         cuda_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=cuda_module.forward\n",
    "#         )\n",
    "#         torch_output = model.to(device_1)(\n",
    "#             *easy_to_device(inputs, device_1), fn=task.module_fn\n",
    "#         )\n",
    "\n",
    "#     rtol_default = 1e-1\n",
    "#     atol_default = 1e-3\n",
    "\n",
    "#     correct = torch.allclose(\n",
    "#         torch_output.cpu(),\n",
    "#         cuda_output.cpu(),\n",
    "#         rtol=rtol_default,\n",
    "#         atol=atol_default,\n",
    "#     )\n",
    "#     max_diff = torch.max(torch.abs(torch_output.cpu() - cuda_output.cpu())).item()\n",
    "#     print(f\"Tested CUDA kernel - Correct: {correct}, Max Diff: {max_diff}\")\n",
    "\n",
    "#     if correct:\n",
    "#         # Evaluate CUDA kernel performance\n",
    "#         cuda_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=cuda_module.forward)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"cuda_module\": cuda_module,\n",
    "#             },\n",
    "#         )\n",
    "#         cuda_runtime = cuda_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated CUDA kernel - Runtime: {cuda_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate PyTorch baseline performance\n",
    "#         torch_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=task.module_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"task\": task,\n",
    "#             },\n",
    "#         )\n",
    "#         torch_runtime = torch_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated PyTorch baseline - Runtime: {torch_runtime:.3f} ms\")\n",
    "\n",
    "#         # Evaluate torch compile performance\n",
    "#         torch_fn = task.module_fn\n",
    "#         compile_fn = torch.compile(torch_fn, mode=\"max-autotune\")\n",
    "#         torch_compile_timer = Timer(\n",
    "#             stmt=\"model(*inputs, fn=compile_fn)\",\n",
    "#             globals={\n",
    "#                 \"model\": model.to(device_1),\n",
    "#                 \"inputs\": easy_to_device(inputs, device_1),\n",
    "#                 \"compile_fn\": compile_fn,\n",
    "#             },\n",
    "#         )\n",
    "\n",
    "#         torch_compile_runtime = torch_compile_timer.timeit(args.rep_time).mean * 1000\n",
    "#         print(f\"Evaluated torch compile - Runtime: {torch_compile_runtime:.3f} ms\")\n",
    "\n",
    "#         print(f\"Speedup over PyTorch: {torch_runtime/cuda_runtime:.2f}x\")\n",
    "#         print(f\"Speedup over torch compile: {torch_compile_runtime/cuda_runtime:.2f}x\")\n",
    "\n",
    "#         import json\n",
    "\n",
    "#         # Store the speedup times as a json file\n",
    "#         file_path = os.path.join(os.path.dirname('speedups'), f\"{op_name}.json\")\n",
    "#         with open(file_path, \"w\") as f:\n",
    "#             json.dump(\n",
    "#                 {\n",
    "#                     \"max_diff\": max_diff,\n",
    "#                     \"cuda_runtime\": cuda_runtime,\n",
    "#                     \"torch_runtime\": torch_runtime,\n",
    "#                     \"torch_compile_runtime\": torch_compile_runtime,\n",
    "#                     \"speedup_over_pytorch\": torch_runtime / cuda_runtime,\n",
    "#                     \"speedup_over_torch_compile\": torch_compile_runtime / cuda_runtime,\n",
    "#                 },\n",
    "#                 f,\n",
    "#             )\n",
    "#         print(f\"Speedup times stored in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(df_l1.iloc[0].Op_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code from KernelBench. Evaluation seems to work with this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using ./cache_dir as PyTorch extensions root...\n",
      "The input conditions for extension module elementwise_add have changed. Bumping to version 4 and re-building as elementwise_add_v4...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file ./cache_dir/elementwise_add/build.ninja...\n",
      "/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module elementwise_add_v4...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=elementwise_add_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /DISK/cs234_cuda/cache_dir/elementwise_add/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=elementwise_add_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/TH -isystem /DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /DISK/conda_envs/cs234/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /DISK/cs234_cuda/cache_dir/elementwise_add/cuda.cu -o cuda.cuda.o \n",
      "[3/3] c++ main.o cuda.cuda.o -shared  -L/DISK/conda_envs/cs234/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o elementwise_add_v4.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module elementwise_add_v4...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    # randomly generate input tensors based on the model architecture\n",
    "    a = torch.randn(1, 128).cuda()\n",
    "    b = torch.randn(1, 128).cuda()\n",
    "    return [a, b]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    # randomly generate tensors required for initialization based on the model architecture\n",
    "    return []\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# Define the custom CUDA kernel for element-wise addition\n",
    "elementwise_add_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        out[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "    auto size = a.numel();\n",
    "    auto out = torch::zeros_like(a);\n",
    "\n",
    "    const int block_size = 256;\n",
    "    const int num_blocks = (size + block_size - 1) / block_size;\n",
    "\n",
    "    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n",
    "\n",
    "    return out;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "elementwise_add_cpp_source = (\n",
    "    \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code for element-wise addition\n",
    "elementwise_add = load_inline(\n",
    "    name=\"elementwise_add\",\n",
    "    cpp_sources=elementwise_add_cpp_source,\n",
    "    cuda_sources=elementwise_add_source,\n",
    "    functions=[\"elementwise_add_cuda\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.elementwise_add = elementwise_add\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.elementwise_add.elementwise_add_cuda(a, b)\n",
    "    \n",
    "a, b = get_inputs()\n",
    "torchm = Model()\n",
    "cudam = ModelNew()\n",
    "torch.allclose(torchm.forward(a,b), cudam.forward(a,b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def module_fn(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single square matrix multiplication (C = A * B).\n",
    "\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix A of shape (N, N).\n",
    "        B (torch.Tensor): Input matrix B of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output matrix C of shape (N, N).\n",
    "    \"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model that performs a single square matrix multiplication (C = A * B)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor, fn=module_fn) -> torch.Tensor:\n",
    "        return fn(A, B)\n",
    "\n",
    "\n",
    "N = 2048\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    A = torch.randn(N, N).cuda()\n",
    "    B = torch.randn(N, N).cuda()\n",
    "    return [A, B]\n",
    "\n",
    "\n",
    "def get_init_inputs():\n",
    "    return []  # No special initialization inputs needed\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cuda_source = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n",
    "\n",
    "__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "\n",
    "    float C_value = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n",
    "        // Load tiles into shared memory\n",
    "        if (row < N && m * TILE_SIZE + tx < N)\n",
    "            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N && m * TILE_SIZE + ty < N)\n",
    "            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Compute partial product\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            C_value += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write the result\n",
    "    if (row < N && col < N)\n",
    "        C[row * N + col] = C_value;\n",
    "}\n",
    "\n",
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n",
    "    CHECK_INPUT(A);\n",
    "    CHECK_INPUT(B);\n",
    "    CHECK_FLOAT(A);\n",
    "    CHECK_FLOAT(B);\n",
    "\n",
    "    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n",
    "    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n",
    "\n",
    "    int64_t N = A.size(0);\n",
    "\n",
    "    auto C = torch::zeros({N, N}, A.options());\n",
    "\n",
    "    const float* A_data = A.data_ptr<float>();\n",
    "    const float* B_data = B.data_ptr<float>();\n",
    "    float* C_data = C.data_ptr<float>();\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
    "\n",
    "    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n",
    "\n",
    "    // Check for kernel launch errors\n",
    "    C10_CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_cpp_source = (\n",
    "    \"torch::Tensor forward(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code \n",
    "cuda_mod = load_inline(\n",
    "    name=\"matmul\",\n",
    "    cpp_sources=cuda_cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"forward\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cuda_mod = cuda_mod\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.cuda_mod.forward(a, b)\n",
    "\n",
    "# QWEN 7B GENERATED\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# # Define the custom CUDA kernel for matrix multiplication\n",
    "# matmul_kernel_source = \"\"\"\n",
    "# #include <torch/extension.h>\n",
    "# #include <cuda_runtime.h>\n",
    "\n",
    "# __global__ void matmul_kernel(const float* a, const float* b, float* out, int n) {\n",
    "#     int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "#     int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "#     float sum = 0.0f;\n",
    "#     for (int k = 0; k < n; k++) {\n",
    "#         sum += a[row * n + k] * b[k * n + col];\n",
    "#     }\n",
    "#     out[row * n + col] = sum;\n",
    "# }\n",
    "\n",
    "# torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {\n",
    "#     int n = a.size(1);\n",
    "#     int m = b.size(0);\n",
    "#     int k = b.size(1);\n",
    "\n",
    "#     auto out_size = m * k;\n",
    "#     auto out = torch::zeros(m, k, sizeof(float));\n",
    "\n",
    "#     const int block_size = 256;\n",
    "#     const int num_blocks = (out_size + block_size - 1) / block_size;\n",
    "\n",
    "#     matmul_kernel<<<num_blocks, block_size>>>(\n",
    "#         a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), n);\n",
    "\n",
    "#     return out;\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# matmul_cpp_source = (\n",
    "#     \"torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);\"\n",
    "# )\n",
    "\n",
    "# # Compile the inline CUDA code for matrix multiplication\n",
    "# matmul = load_inline(\n",
    "#     name=\"matmul\",\n",
    "#     cpp_sources=matmul_cpp_source,\n",
    "#     cuda_sources=matmul_kernel_source,\n",
    "#     functions=[\"matmul_cuda\"],\n",
    "#     verbose=True,\n",
    "#     extra_cflags=[\"-O3\"],\n",
    "#     extra_ldflags=[\"\"],\n",
    "# )\n",
    "\n",
    "# class ModelNew(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.matmul = matmul\n",
    "\n",
    "#     def forward(self, a, b):\n",
    "#         return self.matmul.matmul_cuda(a, b)\n",
    "    \n",
    "torch_mod = Model()\n",
    "cuda_mod = ModelNew()\n",
    "\n",
    "a, b = get_inputs()\n",
    "# a, b, = torch.eye(N).cuda(), torch.eye(N).cuda()\n",
    "print(torch.allclose(torch_mod.forward(a, b), cuda_mod.forward(a, b), rtol=1e-1, atol=1e-3))\n",
    "torch_mod.forward(a, b), cuda_mod.forward(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1_samples.iloc[2].CUDA_Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(l1_samples.iloc[2].PyTorch_Code_Functional)\n",
    "cuda_src = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel for batched matrix multiplication: C = A * B\n",
    "// Shapes: A (batch_size, M, K), B (batch_size, K, N), C (batch_size, M, N)\n",
    "__global__ void bmm_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    float* __restrict__ C,\n",
    "    int batch_size,\n",
    "    int M,\n",
    "    int K,\n",
    "    int N\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * M * N;\n",
    "    if (idx >= total) return;\n",
    "\n",
    "    int b = idx / (M * N);\n",
    "    int remainder = idx % (M * N);\n",
    "    int m = remainder / N;\n",
    "    int n = remainder % N;\n",
    "\n",
    "    float val = 0.0f;\n",
    "    for (int k = 0; k < K; k++) {\n",
    "        val += A[b * M * K + m * K + k] * B[b * K * N + k * N + n];\n",
    "    }\n",
    "    C[b * M * N + m * N + n] = val;\n",
    "}\n",
    "\n",
    "torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B) {\n",
    "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
    "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
    "    TORCH_CHECK(A.dim() == 3, \"A must be 3D\");\n",
    "    TORCH_CHECK(B.dim() == 3, \"B must be 3D\");\n",
    "    TORCH_CHECK(A.size(0) == B.size(0), \"Batch sizes must match\");\n",
    "    TORCH_CHECK(A.size(2) == B.size(1), \"Inner dimensions (K) must match\");\n",
    "\n",
    "    int batch_size = A.size(0);\n",
    "    int M = A.size(1);\n",
    "    int K = A.size(2);\n",
    "    int N = B.size(2);\n",
    "\n",
    "    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n",
    "    auto C = torch::zeros({batch_size, M, N}, options);\n",
    "\n",
    "    int total = batch_size * M * N;\n",
    "    const int threads = 256;\n",
    "    int blocks = (total + threads - 1) / threads;\n",
    "\n",
    "    bmm_kernel<<<blocks, threads>>>(\n",
    "        A.data_ptr<float>(),\n",
    "        B.data_ptr<float>(),\n",
    "        C.data_ptr<float>(),\n",
    "        batch_size, M, K, N\n",
    "    );\n",
    "\n",
    "    return C;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_cpp_source = (\n",
    "    \"torch::Tensor forward_bmm(torch::Tensor A, torch::Tensor B);\"\n",
    ")\n",
    "\n",
    "# Compile the inline CUDA code \n",
    "cuda_mod = load_inline(\n",
    "    name=\"bmm\",\n",
    "    cpp_sources=cuda_cpp_source,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=[\"forward_bmm\"],\n",
    "    verbose=True,\n",
    "    extra_cflags=[\"\"],\n",
    "    extra_ldflags=[\"\"],\n",
    ")\n",
    "\n",
    "\n",
    "class ModelNew(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cuda_mod = cuda_mod\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.cuda_mod.forward_bmm(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_inputs()\n",
    "a = a.cuda()\n",
    "b = b.cuda()\n",
    "\n",
    "torchm = Model()\n",
    "cudam = Model()\n",
    "print(torch.allclose(torchm.forward(a, b), cudam.forward(a, b), rtol=1e-1, atol=1e-3))\n",
    "torchm.forward(a, b), cudam.forward(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- ~~evaluation stuff~~\n",
    "- prompting qwen for good outputs \n",
    "- ~~KernelBench method - NA~~\n",
    "- coding the RL portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL portion\n",
    "Below implements a GRPO like method (calculating advantage from reward - mean reward in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # rank of the low-rank matrices\n",
    "    lora_alpha=16,            # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust based on your model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Wrap the model with LoRA. This freezes the base model parameters and injects trainable adapters.\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 1,778,177,536 || trainable%: 0.0613\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, SGD, Adafactor\n",
    "import torch.optim\n",
    "from prompting import prompt_generate_custom_cuda_from_prompt_template, prompt_generate_reprompt\n",
    "from reward_model import reward\n",
    "import gc\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "\n",
    "# the batch size hyperparameters\n",
    "batch_size = 2          # size of our batch (number of prompts)\n",
    "reprompts = 2           # number of times we try while including the error message. includes the first prompt\n",
    "\n",
    "# PPO/training hyperparameters\n",
    "clip_range = 0.2        # clipping range for PPO\n",
    "ppo_epochs = 10          # number of PPO updates per batch\n",
    "num_iterations = 1      # total training iterations\n",
    "log_prob_min_ratio = -10\n",
    "log_prob_max_ratio = 5\n",
    "lr = 1e-5\n",
    "\n",
    "# text generation hyperparameters\n",
    "temperature = 1\n",
    "max_new_tokens = 3_000\n",
    "\n",
    "# the PyTorch Code Module we're tackling.\n",
    "problem_id = 0\n",
    "pytorch_str = l1_samples.iloc[problem_id]['PyTorch_Code_Module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a mask from a list of ignore ranges.\n",
    "\n",
    "def create_ignore_mask(seq_len, attention_mask, ignore_ranges, dtype=torch.bfloat16):\n",
    "    \"\"\"\n",
    "    Create a 1D mask (length seq_len) where tokens in any ignore range are set to 0,\n",
    "    and others are 1.\n",
    "    ignore_ranges: list of tuples (start, end) with end not included.\n",
    "    \"\"\"\n",
    "    if attention_mask is not None:\n",
    "        assert len(attention_mask.shape) == 1\n",
    "        mask = F.pad(attention_mask.to(dtype), (0, seq_len - attention_mask.shape[0]), mode='constant', value=1)\n",
    "    else:\n",
    "        mask = torch.ones(seq_len, dtype=dtype)\n",
    "    for (start, end) in ignore_ranges:\n",
    "        # Ensure the ignore indices are within bounds\n",
    "        start = max(0, start)\n",
    "        end = min(seq_len, end)\n",
    "        mask[start:end] = 0.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration num: 0\n",
      "\treprompt attempt 0\n"
     ]
    }
   ],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=lr)\n",
    "# optimizer = SGD(model.parameters(), lr=lr)\n",
    "optimizer = Adafactor(model.parameters(), lr=lr)\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f'iteration num: {iteration}')\n",
    "    batch_outputs = []\n",
    "    batch_rewards = [0 for _ in range(batch_size)]\n",
    "\n",
    "    ################################\n",
    "    # (old policy): generate data. #\n",
    "    ################################\n",
    "\n",
    "    batch_outputs = [\"\" for _ in range(batch_size)]\n",
    "    prompt = prompt_generate_custom_cuda_from_prompt_template(pytorch_str, add_think=False)\n",
    "    prompts = tokenizer([prompt for _ in range(batch_size)]) # basically dictionary of keys 'input_ids', 'attention_mask'. in list form\n",
    "    pads = [0 for _ in range(batch_size)]\n",
    "    tokens_to_ignore_later = [[(0, len(ids))] for ids in prompts['input_ids']]\n",
    "    # format: list of list of tuples. first index is the number of the item within the batch. \n",
    "    # for each batch, list of tuples of indices to ignore (start, end) where start is included and end is NOT.\n",
    "\n",
    "    # variables required for the old_log_prob calculation\n",
    "    gen_ids = None\n",
    "    last_attention_mask = None\n",
    "\n",
    "    for idx in range(reprompts):\n",
    "        print(f'\\treprompt attempt {idx}')\n",
    "        inputs = tokenizer.pad(prompts, padding=True, return_tensors='pt').to(device)\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        if idx == reprompts - 1: # used to calculate the number of pad tokens for the last iteration\n",
    "            last_attention_mask = inputs.attention_mask\n",
    "\n",
    "        # take only the tokens after the inputs\n",
    "        batch_outputs = [tokenizer.decode(ids[inputs.input_ids.shape[1]:], skip_special_tokens=True) for ids in gen_ids]\n",
    "        for i, output_txt in enumerate(batch_outputs):\n",
    "            print(f'\\t\\tcalculating reward for item {i} in batch')\n",
    "            r, msg = reward(pytorch_str, output_txt)\n",
    "            batch_rewards[i] += r\n",
    "            reprompt = prompt_generate_reprompt(msg)\n",
    "\n",
    "            output_ids = tokenizer(batch_outputs[i]).input_ids\n",
    "            reprompt_ids = tokenizer(reprompt).input_ids\n",
    "\n",
    "            orig_tokens = tokens_to_ignore_later[i][-1][1]\n",
    "            output_tokens = len(output_ids)\n",
    "            reprompt_tokens = len(reprompt_ids)\n",
    "\n",
    "            prompts.input_ids[i].extend(output_ids)\n",
    "            prompts.input_ids[i].extend(reprompt_ids)\n",
    "            prompts.attention_mask[i].extend([1] * (output_tokens + reprompt_tokens))\n",
    "            tokens_to_ignore_later[i].append((orig_tokens + output_tokens, orig_tokens + output_tokens + reprompt_tokens))\n",
    "        \n",
    "    # compute log probs. ignore any instances of pad_token using the attention mask\n",
    "    with torch.no_grad():\n",
    "        outputs = model(gen_ids) # this uses the last instance of gen_ids\n",
    "        log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "        gen_log_probs = log_probs.gather(2, gen_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Create a masked log probability for each batch element.\n",
    "        old_log_probs_list = []\n",
    "        batch_size, seq_len = gen_ids.shape\n",
    "        for i in range(batch_size):\n",
    "            # Create a mask for this batch element using tokens_to_ignore_later.\n",
    "            # tokens_to_ignore_later[i] is a list of tuples (start, end) to ignore.\n",
    "            mask = create_ignore_mask(seq_len, last_attention_mask[i], tokens_to_ignore_later[i], dtype=dtype).to(gen_log_probs.device)\n",
    "            masked_log_probs = gen_log_probs[i] * mask\n",
    "            # Sum over the sequence to get a scalar log-prob for this example.\n",
    "            old_log_prob = masked_log_probs.sum()\n",
    "            old_log_probs_list.append(old_log_prob)\n",
    "        \n",
    "        old_log_probs_tensor = torch.stack(old_log_probs_list)\n",
    "    \n",
    "    print()\n",
    "    print(f'rewards: {batch_rewards}')\n",
    "    if batch_size > 1 and all(r == batch_rewards[0] for r in batch_rewards):\n",
    "        print(f'\\tthis batch had the same rewards. not performing PPO on this batch.')\n",
    "        continue\n",
    "    print(f'old log probs: {old_log_probs_tensor}')\n",
    "    print('done with generations in old policy')\n",
    "    print()\n",
    "\n",
    "    rewards_tensor = torch.tensor(batch_rewards, dtype=dtype).to(device)\n",
    "\n",
    "    ####################################\n",
    "    # PPO Update Loop with Masking   #\n",
    "    ####################################\n",
    "    # In the PPO update loop, it is important that we compute the new log probs\n",
    "    # over the full concatenated sequence (i.e. the updated prompts) and then apply the same mask.\n",
    "\n",
    "    for _ in range(ppo_epochs):\n",
    "        print(f'\\tppo_epoch: {_}')\n",
    "\n",
    "        # Process each batch element individually.\n",
    "        for i, output_text in enumerate(batch_outputs):\n",
    "            # Instead of re-tokenizing only the new output, use the entire sequence stored in prompts[i]\n",
    "            full_ids = torch.tensor(prompts.input_ids[i], device=device).unsqueeze(0)\n",
    "            outputs = model(full_ids)\n",
    "            log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "            gen_log_probs = log_probs.gather(2, full_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            # Create the ignore mask using tokens_to_ignore_later for this batch item. Note None since prompts[i] shouldn't have any padding tokens, thus no attention mask necessary\n",
    "            mask = create_ignore_mask(full_ids.shape[1], None, tokens_to_ignore_later[i], dtype=dtype).to(device)\n",
    "            # Compute the masked sequence log probability.\n",
    "            sequence_log_prob = (gen_log_probs[0] * mask).sum()\n",
    "\n",
    "            ratio = torch.exp(torch.clamp(sequence_log_prob - old_log_probs_tensor[i], log_prob_min_ratio, log_prob_max_ratio))\n",
    "            advantage = rewards_tensor[i] - rewards_tensor.mean()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * advantage\n",
    "            loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_val = loss.item()\n",
    "            del full_ids, outputs, log_probs, gen_log_probs, sequence_log_prob, ratio, advantage, surr1, surr2, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f'\\t\\tloss = {loss_val:.4f}')\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs234",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
