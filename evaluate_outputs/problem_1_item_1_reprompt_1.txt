Okay, let me try again. The initial attempt had the CUDA kernel incorrectly formatted and did not follow the required structure. Now, I'll ensure proper indentation and function structure.

The optimized code now has proper CUDA kernel formatting, following the structure of the provided examples.
</think>

```
cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <occupancy.h>

__global__ void matmul_fused_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    float Cij = 0;
    for (int k = 0; k < K; ++k) {
        Cij += A[i * K + k] * B[k * N + j];
    }
    C[i * N + j] = Cij;
}

torch::Tensor forward(torch::Tensor A, torch::Tensor B) {
    auto M = A.size(0);
    auto K = A.size(1);
    auto N = B.size(1);
    int lda = K, ldb = N, ldc = N;
    int size = M * N;
    
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    matmul_fused_kernel<<<num_blocks>>>(A.data_ptr(), B.data_ptr(), C.data_ptr(), M, K, N);
    
    return C;
}
"""

cpp_source = """
torch::Tensor forward(torch::Tensor A, torch::Tensor B);
"""
```