

To optimize the 3D tensor-matrix multiplication using CUDA, we'll replace the PyTorch `matmul` operation with a custom CUDA kernel. This will speed up the computation by offloading the matrix multiplication to the GPU.

```cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matmul_3d_kernel(const float* A, const float* B, float* out, int N, int M, int K, int L) {
    int blocksize = 256;
    int grid_size = (K + blocksize - 1) / blocksize;
    
    for (int i = (blockIdx.x) * blocksize; i < N; i += blocksize) {
        for (int j = (blockIdx.y) * blocksize; j < M; j += blocksize) {
            float sum = 0.0f;
            for (int k = 0; k < K; k++) {
                sum += A[i * K + k] * B[k * L + j];
            }
            out[i * M + j] = sum;
        }
    }
}

torch::Tensor forward(torch::Tensor A, torch::Tensor B) {
    int N = A.size(0);
    int M = A.size(1);
    int K = A.size(2);
    int L = B.size(1);

    auto size = N * M * L;
    auto out = torch::zeros(N, M, L, {static_cast<int64_t>(N)}, {static_cast<int64_t>(M)}, {static_cast<int64_t>(L)}, {0});

    int threadsPerBlock = 256;
    int blocksPerGrid = (K + threadsPerBlock - 1) / threadsPerBlock;

    matmul_3d_kernel<<<blocksPerGrid, threadsPerBlock>>>(A.data_ptr(), B.data_ptr(), out.data_ptr(), N, M, K, L);

    return out;
}
"""

cpp_source = (
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B);"
)
```