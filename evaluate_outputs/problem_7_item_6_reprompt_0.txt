

To optimize the matrix multiplication operation with irregular tensor shapes, we've designed a custom CUDA kernel that efficiently handles the differing dimensions by processing blocks that fit within the maximum warp size. This approach allows us to fully utilize the GPU's parallel processing capabilities while maintaining correctness.

```cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < M) {
        int rowStart = i * K;
        for (int j = 0; j < N; j++) {
            float sum = 0.0f;
            for (int k = 0; k < K; k++) {
                sum += A[rowStart + k] * B[k * N + j];
            }
            C[i * N + j] = sum;
        }
    }
}

torch::Tensor forward(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(1);
    
    auto device = A.device();
    auto group_count = grid(dim3(256), 1);
    auto group = group_count.index(0);
    auto (blockSize, threadsPerBlock) = group gridSize();
    
    auto C = torch::empty(M, N, device=device);
    auto grid_size = (M + threadsPerBlock - 1) / threadsPerBlock;

    matmul_kernel<<<grid_size, threadsPerBlock>>>(
        A.data(), B.data(), C.data(), M, K, N
    );

    return C;
}
"""

cpp_source = (
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B);"
)
```