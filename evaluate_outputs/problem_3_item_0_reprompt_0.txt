

To optimize the matrix-vector multiplication model using custom CUDA kernels, we implement a kernel for the GEMM (General Matrix Multiply) operation. This approach improves performance by reducing memory transfers and leveraging the GPU's parallel processing power.

Here's the CUDA implementation:

cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_kernel(const float* A, const float* B, float* C, 
                            int M, int K, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    for (int j = 0; j < N; j++) {
        int k = i * N + j;
        C[k] = 0.0f;
        for (int l = 0; l < K; l++) {
            C[k] += A[blockRow(i, l) * K + l] * B[blockCol(j, l) * N + l];
        }
    }
}

torch::Tensor forward(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(1);
    int size = M * N;

    float* d_A = A.data_ptr<float>();
    float* d_B = B.data_ptr<float>();
    float* d_C = torch::emptyCUDA<float>(M, N).data_ptr();

    const int block_size = 256;
    const int num_blocks = (M + block_size - 1) / block_size;

    gemm_kernel<<<num_blocks, block_size>>>(d_A, d_B, d_C, M, K, N);

    return torch:: Tensor(C, C.device(), C.size(), C stride(), C.dtype(), C layouts());
}
"""

cpp_source = (
    "torch::Tensor forward(torch::Tensor A, torch::Tensor B);"
)