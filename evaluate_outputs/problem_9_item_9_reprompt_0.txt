

To optimize the 3D tensor-matrix multiplication in CUDA, we'll fuse the multiplication and transposition into a single kernel for efficiency. This reduces memory accesses and improves performance.

```cuda_source
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matmulkernel(
    const float* __global__ A,
    const float* __global__ B,
    float* __global__ C,
    int N,
    int M,
    int L,
    int K) {
    int tx = threadIdx.x;
    int block = blockIdx.x;
    int stride = blockDim.x * gridDim.y;
    int row = block * stride + tx;
    if (row < N) {
        for (int k = 0; k < K; k++) {
            C[row * M + tx * L + k] = A[row * M + k] * B[k * L + tx];
        }
    }
}

torch::Tensor forward(
    torch::Tensor A,
    torch::Tensor B) {
    int num_threads = 256;
    int num_block = ((int)(A.numel() / num_threads) + 1) / 2;
    matmulkernel<<<num_block, num_threads>>>(A.data_ptr(), B.data_ptr(), B.numel() / L, A.size(1), M, L, K);
    return A;
}
```

```cpp_source
torch::Tensor forward(torch::Tensor A, torch::Tensor B);
```